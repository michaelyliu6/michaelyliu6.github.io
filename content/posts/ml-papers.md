+++
title = 'ML Papers'
date = 2024-12-14T07:07:07+01:00
draft = false
+++

# AI Alignment
## Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming
By Anthropic (Jan 2025) https://arxiv.org/pdf/2501.18837
<img src="/assets/images/ml-papers/constitutional-classifiers.png">

## Alignment Faking in Large Language Models
By Anthropic (Dec 2024) https://arxiv.org/pdf/2412.14093
<img src="/assets/images/ml-papers/alignment-faking.png">

## Constitutional AI: Harmlessness from AI Feedback
By Anthropic (Dec 2022) https://arxiv.org/pdf/2212.08073
<img src="/assets/images/ml-papers/constitutional-ai.png">

# Reasoning
## DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
By DeepSeek (Jan 2025) https://arxiv.org/pdf/2501.12948
<img src="/assets/images/ml-papers/deepseek-r1.png">

## DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
By DeepSeek (Apr 2024) https://arxiv.org/pdf/2401.06066
<img src="/assets/images/ml-papers/grpo.png">

# Reinforcement Learning
## Playing Atari with Deep Reinforcement Learning
By DeepMind (December 2013) https://arxiv.org/pdf/1312.5602

