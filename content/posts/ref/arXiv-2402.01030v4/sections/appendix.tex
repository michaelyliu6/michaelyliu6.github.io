\appendix
\newpage
\renewcommand{\thefigure}{A.\arabic{figure}}
\renewcommand{\thetable}{A.\arabic{table}}
\newcounter{tcolorboxcounter}
\renewcommand{\thetcolorboxcounter}{\arabic{tcolorboxcounter}}
\renewcommand{\thetcolorboxcounter}{A.\arabic{tcolorboxcounter}}  % Add this line
\onecolumn


\begin{table}[h]
    % \vspace{-0.5cm}
    \centering
    \caption{Example of actions for re-purposed API-Bank \citep{li2023apibank} and \evalname.}
    \resizebox{0.7\columnwidth}{!}{%
    \begin{tabular}{@{} l p{8cm} @{}}
        \toprule
        \textbf{Format} & \textbf{Action} \\
        \midrule
        \multirow{2}*{\approach} & \texttt{AddAgenda(content="Meeting with John", time="2023-10-26 09:00:00")} \\
        \hline
        \multirow{3}*{JSON} & \texttt{ \{"action": "AddAgenda", "content": "Meeting with John", "time": "2023-10-26 09:00:00"\} } \\ 
        \hline
        \multirow{2}*{Text} & \texttt{Action: AddAgenda, content: Meeting with John, time: 2023-10-26 09:00:00} \\
        \bottomrule
        \end{tabular}
    }
    \label{tab:apibank-action-example}
    \vspace{-0.3cm}
\end{table}

\begin{table*}[!htb]
\centering
\caption{Comparison between \evalname and existing tool-use evaluation benchmark.
}

\resizebox{\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|ccccc}
\toprule
\textbf{Benchmark} & \evalname & ToolBench & APIBench & API-Bank & ToolBench \\
& (This work) & \citep{Qin2023ToolLLMFL} & \citep{Patil2023GorillaLL} & \citep{li2023apibank} & \citep{xu2023tool} \\
\midrule
Requiring multi-turn interaction       & \greencheck & \greencheck & \redwrong & \redwrong & \redwrong \\
Multiple tools                         & \greencheck & \greencheck & \greencheck & \greencheck & \greencheck \\
Evaluation                     & Answer Match & LLM Evaluator & AST Tree Match & API-Call Match & Test Case \\
No dependency on external API$^*$        & \greencheck & \redwrong & \redwrong & \greencheck & \redwrong \\
Supported API Action Format             & \approach \& JSON \& Text & JSON & \approach & JSON & \approach \\
\bottomrule
\end{tabular}
\begin{tablenotes}
    \item[*] Whether to rely on external API (e.g., RapidAPI, Google Sheet) hosted by a third party. The availability of such third-party APIs can greatly impact evaluation results (e.g., low API-calling performance not because the model is bad but rather because the API required is not accessible).
\end{tablenotes}
\end{threeparttable}
}
\label{tab:tool_bench_comparison}
\end{table*}



\begin{table*}[!h]
    \centering
\caption{Ablation study results. The best results are \textbf{bolded}, and the second-best results are \underline{underlined}.
ID and OD stand for in-domain and out-of-domain evaluation correspondingly.
Overall averaged performance normalizes the MT-Bench score to be consistent with other tasks and excludes in-domain tasks for fair comparison.
}
\label{tab:ablation_study_results}

\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{@{} lc rr m{0.01em} rr m{0.01em} rrrr m{0.01em} r@{}}
\toprule
& & \multicolumn{5}{c}{\textbf{Agent Tasks}}                                                        && \multicolumn{4}{c}{\textbf{Generic LLM Tasks}} && \textbf{Overall} \\
 \cmidrule{3-7}
& & \multicolumn{2}{c}{\textbf{Code as Action}} && \multicolumn{2}{c}{\textbf{Text as Action (OD)}} && \multicolumn{4}{c}{\textbf{(OD)}} && \textbf{Average} \\
  \cmidrule{3-4}
  \cmidrule{6-7}
  \cmidrule{9-12}
  Model & Size & MINT (ID) & MINT (OD) && Miniwob++ & SciWorld && MMLU & HumanEval & GSM8K & MTBench \\
\midrule

\modelname (Llama2-based) & 7B & $\mathbf{51.3}$ & $\mathbf{20.4}$ &  & \underline{$25.5$} & $\mathbf{17.6}$ &  & $\mathbf{50.6}$ & \underline{$18.1$} & $\mathbf{38.3}$ & $\mathbf{7.5}$ &  & $\mathbf{35.1}$ \\
w/o \approach & 7B & $17.0$ & $15.5$ &  & $\mathbf{36.4}$ & $16.9$ &  & \underline{$49.5$} & $14.7$ & \underline{$36.0$} & \underline{$7.2$} &  & \underline{$34.5$} \\
w/o general conversations & 7B & \underline{$29.2$} & \underline{$15.9$} &  & $0.0$ & \underline{$17.1$} &  & $46.4$ & $\mathbf{19.7}$ & $20.6$ & $4.1$ &  & $22.9$ \\

\midrule

\modelname (Mistral-based) & 7B & $\mathbf{57.4}$ & $\mathbf{32.4}$ &  & \underline{$46.2$} & \underline{$15.9$} &  & \underline{$59.1$} & $\mathbf{34.7}$ & \underline{$58.0$} & \underline{$8.2$} &  & $\mathbf{46.8}$ \\
w/o \approach & 7B & $32.9$ & \underline{$23.0$} &  & $\mathbf{47.8}$ & $\mathbf{17.0}$ &  & $\mathbf{59.9}$ & \underline{$33.2$} & $\mathbf{59.5}$ & $\mathbf{8.3}$ &  & \underline{$46.2$} \\
w/o general conversations & 7B & \underline{$50.5$} & $13.9$ &  & $0.0$ & $11.0$ &  & $52.4$ & $27.9$ & $26.8$ & $2.6$ &  & $22.6$ \\
\bottomrule
\end{tabular}
\end{adjustbox}
% }
\end{table*}



\section{Comparison with Work that Uses Code Generation for Problem-solving}
\label{sec:prior_code_action_work}

In this section, we discuss the fundamental differences between \approach and prior work that prompt LLM to generate code for problem-solving. 
% 
Existing work have explored using code generation for task-solving in different domains, for example, Code4Struct \citep{wang-etal-2023-code4struct} for structured prediction, PaL \citep{gao2023pal} for math reasoning, Meta-GPT \citep{hong2023metagpt} for multi-agent collaboration, code-as-policy \citep{codeaspolicies2022} for robot control, ViperGPT \citep{surismenon2023vipergpt} for visual question answering, Voyager \citep{wang2023voyager} for playing games, Data Interpreter \citep{hong2024data} for data science tasks, etc.

Most prior work generates code (i.e., a static sequence of actions) in a single-turn setting and cannot dynamically readjust action on new observation: It is considered a failure when the model-generated code fails to solve a task on the \textit{first attempt}.
% 
This setting overlooks the potential of environmental observation (e.g., code execution results) that might benefit future action and overall decision (e.g., dynamically adjusting subsequent code after observing intermediate code execution results, fixing erroneous code after seeing an error message).
% 
That is, the generated code is a static sequence of actions that cannot be dynamically re-adjusted on the fly by incorporating new observations.
% 
Such a single-turn setting makes it challenging to scale to more challenging problems since even expert human programmers usually cannot write functionally correct code in the first pass.
% 
On the other hand, \approach is a multi-turn interaction agent framework that allows dynamic adjustment of prior actions or emitting new actions by design (\sref{sec:codeact_definition}, \fref{fig:llm_agent_framework}) and is compatible with any form of textual observation (e.g., tool execution output, automated feedback) from the environment.
% 
Beyond being compatible with environmental observation, our instruction tuning dataset \dataname specifically collects data for multi-turn self-improving, offering a practical solution to enhance LLM's multi-turn self-improving process.


In addition, previous approaches require heavy prompt engineering and crafting of few-shot demonstrations to tailor LLMs to a particular domain or task (e.g., robot control~\citep{codeaspolicies2022}) since the backbone LLMs are not specially optimized for dynamic planning and decision making.
% 
In contrast, in this work, we propose the \approach framework that uses executable Python code to consolidate LLM agents' actions into unified action space and collect \dataname on a diverse array of tasks (e.g., information seeking, tabular reasoning, robot planning, etc) to make the trained model, \modelname, easily scale to diverse tasks and domains with minimal human efforts as shown in \sref{sec:llm_agent_evaluation}. 

One notable exception among prior work is Voyager \citep{wang2023voyager}, which performs iterative prompting in a constrained action space of \textit{function definitions} to fix code errors.
% 
Different from \approach, such setting disallows dynamic re-adjustment of \textit{atomic} actions on the fly: In \approach, for a particular task (e.g., craft stone sword in Minecraft), the agent can first execute one line of code (any atomic action or composed functions, e.g., move forward, locate stone), and dynamically produce different actions based on the observation of the first action.
% 
This is challenging for Voyager to achieve: Similar to code-as-policy \citep{codeaspolicies2022}, they generate action (a skill, e.g., craft stone sword) as a Python \textit{function definition} that outlines the entire plan for a task (e.g., multi-step code outlining how you should craft a stone sword and handles for different potential cases, which requires strong domain knowledge).
% 
This imposes significant constraints on the agent's action space and disallows dynamic re-adjustment of \textit{atomic} actions on the fly: That is, the agent can only generate one complete function first (e.g., by imaging all possible cases that might happen when you try to locate stones), execute the entire function, observe the feedback, and update the entire function as action in the subsequent move.
% 
Besides the constrained ability to re-adjust action from environmental observation, they also rely on heavy prompting engineering (a typical drawback discussed above) to provide relevant information (e.g., current state, additional self-critics via prompting) to generate revised code, whereas \approach is situated in a setting that requires no prompt engineering efforts: the context window of LLM only contains its \textit{past actions and observations} and does not require human efforts to filter for relevant information.

Similar to \approach, concurrent work OpenCodeInterpreter \citep{opencodeinterpreter}, with a specific focus on competitive code generation questions, collects code-debugging trajectories to improve an LLM's iterative code debugging performance. However, its applicability to general LLM agent tasks remains unknown.



\section{Comparison with TaskWeaver}
\label{sec:taskweaver}
In the landscape of unifying the action space of LLM agents, our work represents a leap over the previous initiative, TaskWeaver~\citep{qiao2023taskweaver}. 
% 
While TaskWeaver deserves acknowledgment for initially integrating code into the action space of LLM agents, its exploration remains limited.
% 
This work, primarily characterized by its reliance on a limited set of qualitative examples with close-sourced models as the backbones, fails to harness the full potential of this integration, remaining merely conceptual demonstrations.
% 
Our work transcends mere conceptualization by conducting an extensive and rigorous analysis, clearly quantifying the benefits of code action within LLM agents.
% 
% 
Beyond this, we introduce a unique instruction-tuning dataset \dataname specifically designed to amplify the agent's capabilities in executing code-based actions and an open-source LLM agent \modelname.
% 
These contributions not only extend the work of TaskWeaver but also pave the way for future explorations, offering valuable resources to the open-source community and redefining the potential of LLM agents in practical applications.


\section{General Data Down-sample}
\label{sec:general_data_downsample}

\begin{itemize}
    \item \textbf{ShareGPT} \citep{sharegpt_dataset}: We remove all single-turn conversations, then perform random sub-sample to a desired final number.
    \item \textbf{ShareGPT (GPT-4)} \citep{sharegpt4_dataset}: We do not perform sub-sampling on this dataset.
    \item \textbf{OpenOrca} \citep{OpenOrca}: We select the CoT subset of OpenOrca, then perform a random sub-sample to a desired final number.
    \item \textbf{CapyBara} \citep{capybara_dataset}: We do not perform sub-sampling on this dataset.
\end{itemize}


\section{\modelname Training Details}
\label{sec:model_training_details}

All SFT experiments are performed on one 4xA100 40GB SXM node using a fork of Megatron-LLM \citep{epfmgtrn} with a training throughput of around 9k tokens per second. We use chatML format\footnote{\url{https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md}} for all multi-turn data, and we only calculate and optimize for loss on the assistant response. We pack short instances into longer ones and apply flash attention for training efficiency. 

We train both LLaMA-2 and Mistral LLMs with Tensor Parallel of 4, the learning rate of \texttt{1e-5} with 50 warmup steps and cosine decay (end learning rate of \texttt{1e-6}). We train for five epochs with a batch size of 32. We use the 3rd epoch checkpoint for all our experiments.

\section{Example Prompt for \approach}
\label{sec:codeact_prompt}

This is an example (zero-shot) system prompt used in a deploy instance of \approach where we used chatML format.

The users may optionally include tools descriptions similar to \sref{sec:zeroshot_act_prompt} or including extra in-context examples similar to \sref{sec:interaction_data_gen_prompt}.

\begin{lstlisting}[breaklines]
<|im_start|>system
A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
The assistant can interact with an interactive Python (Jupyter Notebook) environment and receive the corresponding output when needed. The code should be enclosed using "<execute>" tag, for example: <execute> print("Hello World!") </execute>.
The assistant should attempt fewer things at a time instead of putting too much code in one <execute> block. The assistant can install packages through PIP by <execute> !pip install [package needed] </execute> and should always import packages and define variables before starting to use them.
The assistant should stop <execute> and provide an answer when they have already obtained the answer from the execution result. Whenever possible, execute the code for the user using <execute> instead of providing it.
The assistant's response should be concise, but do express their thoughts.
<|im_end|>
\end{lstlisting}

\section{\evalname Prompt}
\label{sec:zeroshot_act_prompt}

\begin{lstlisting}[breaklines]
You have access to the following tools:
{{Tool Definition}}

{{Formatting Instruction}}

Now, let's get started!

Instruction: {{Example: Find the current price of Legendary Wand.}}
Answer in the format of 'xx.xx' (e.g., 12.34).

You can optionally express your thoughts using natural language before your action. For example, 'Thought: I want to use tool_name to do something. Action: <your action to call tool_name> End Action'.
Note that your output should always contain either 'Action:' or 'Answer:', but not both.
When you are done, output the result using 'Answer: your answer'
Please ONLY output the answer (e.g., single number), without any other text.
\end{lstlisting}

Each \texttt{\{\{...\}\}} component above will be substituted with corresponding information.

\subsection{Example of \{\{Tool Definition\}\}}

The following is an example tool definition for web-browsing.

\begin{lstlisting}[breaklines]
[1] click_url: Clicks on a URL. A clickable URL looks like [Clickable '<url_argument>'] in the webpage.
Arguments: url (str).
Returns the rendered content of the webpage after clicking the URL showing on the current rendered page.
    Signature: click_url(url: str) -> str
[2] go_to_previous_page: Goes back to the previous page. It has no arguments.
After going back to the previous page, return the rendered content of the webpage.
    Signature: go_to_previous_page() -> str
[3] scroll_down: Scrolls down the view. It has no arguments.
Returns the rendered content of the webpage after scrolling down.
    Signature: scroll_down() -> str
[4] scroll_up: Scrolls up the view. It has no arguments.
Returns the rendered content of the webpage after scrolling up.
    Signature: scroll_up() -> str
[5] view: Return the current view in string format of the rendered webpage. It has no arguments.
Returns the rendered content of the webpage.
You should call this when you want to see the rendered content of the current webpage.
    Signature: view() -> str
[6] calculator: Evaluates the given expression and returns the result. Accepts a calculation expression as input. For example, "2 + (3 * 4)" will return 14.
    Signature: calculator(expression: str) -> float
\end{lstlisting}

\subsection{Example of \{\{Formatting Instruction\}\}}

Different action format has different formatting instructions.

\subsection{Formatting Instruction for Code as Action}

\begin{lstlisting}[breaklines]
You can use the tools by outputing a block of Python code that invoke the tools.
You may use for-loops, if-statements, and other Python constructs when necessary.
Be sure to print the final answer at the end of your code.
You should begin your tool invocation with 'Action:' and end it with 'End Action'.
Example: 'Action:
tool_name(argument_1)
End Action'
\end{lstlisting}

\subsection{Formatting Instruction for Json as Action}

\begin{lstlisting}[breaklines]
You can use the tools by outputing a JSON object with the following fields:
  - 'tool': the name of the tool
  - 'args': a list of arguments to the tool
You should begin your tool invocation with 'Action:' and end it with 'End Action'.
Example: 'Action: {"tool": "tool_name", "args": ["argument_1"]} End Action'
You can only invoke one tool at a time.
\end{lstlisting}

\subsection{Formatting Instruction for Text as Action}

\begin{lstlisting}[breaklines]
You can use the tools by outputing the tool name followed by its arguments, delimited by commas.
You should begin your tool invocation with 'Action:' and end it with 'End Action'.
Example: 'Action: tool_name, argument_1 End Action'
You can only invoke one tool at a time.
\end{lstlisting}



\section{\approach Interaction Data}

\subsection{Dataset Downsample}
\label{sec:how_to_downsample_dataset}

\begin{table*}[!htb]
\centering
\caption{\dataname components and the number of instances for training trajectory generation.}
\resizebox{\textwidth}{!}{
\begin{tabular}{ll|lr}
\toprule
Domain & Capability & Dataset & \# of Instances \\
\midrule
Web Search & Information seeking through search API & HotpotQA \citep{yang2018hotpotqa} & 3,000 \\
Math Reasoning & Math problem-solving using math Libraries in Python (e.g., \texttt{sympy}) & MATH \citep{hendrycks2021measuring} & 5,586 \\
Code Generation & Self-debug from Python error messages and traceback & APPS \citep{hendrycks2021math} & 4,439 \\
Tabular Reasoning & Tabular Reasoning using \texttt{pandas} and \texttt{sqlite3} (for SQL) library & WikiTableQuestion \citep{pasupat2015compositional} & 3,000 \\
Embodied Planning & Interact with embodied environments through APIs & ALFWorld \citep{shridhar2020alfworld} & 3,553 \\
\bottomrule
\end{tabular}
}
\label{tab:data_training_instances}
\end{table*}

\begin{itemize}
    \item \textbf{Code generation tasks in APPS} \citep{hendrycks2021measuring}: We remove instances without any test case available.

    \item \textbf{Tabular reasoning tasks in WikiTableQuestion} \citep{pasupat2015compositional}: We select a subset of 3000 instances with the largest table size (i.e., sort by number of rows and columns) from the original dataset (14149 instances), and randomly assign 1500 of them to be \texttt{pandas}-based problems, and the rest 1500 to be SQL-based problems.

    \item \textbf{Web search tasks in HotpotQA} \citep{yang2018hotpotqa}: We select the 15661 problems labeled as ``hard" in the original dataset (with 90447 instances), then randomly down-sample them to 3000 problems.

    \item \textbf{Math reasoning in MATH} \citep{hendrycks2021math}: We remove problems with the annotated difficulty lower than 3, which results in 5586 instances as shown in \tref{tab:data_training_instances}.

    \item \textbf{Embodied Planning in ALFWorld} \citep{shridhar2020alfworld}: We did not perform down-sampling for AlfWorld.
    
\end{itemize}


\subsection{Data Selection Heuristic}
\label{sec:data_filter_heuristic}

Given successful task-solving trajectories that have more than 2 turns, we apply the following heuristic to select instances that can promote the code-as-actions, self-improvement, and instruction-following capabilities of LLM agents: 

\begin{itemize}
    \item \textbf{Code-as-Actions:} 
    We exclude trajectories wherein LLM agents do not adhere to the code-as-actions framework, either due to incorrect API invocation or the generation of actions in formats unsuitable for parsing and execution.

    \item \textbf{Self-Improving:} We selectively preserve those trajectories wherein the model initially encounters errors but subsequently rectifies these inaccuracies in later interactions. In addition, we eliminate successful trajectories that exclusively yield errors in all code executions. These are deemed ineffective demonstrations, as our objective is to prevent the model from learning to consistently execute erroneous code while still managing to provide correct answers.
    
    \item \textbf{Instruction-Following: }We remove rare cases where the LLM agents fail to follow the instruction and respond to the user, identified by an odd number of interaction turns.

\end{itemize}

After applying all these heuristics, we obtain 6728 trajectories (out of 6985) from \texttt{gpt-3.5} and \texttt{claude}, and 411 trajectories (out of 413) from \texttt{gpt-4-0613}.

\subsection{Example of Trajectory Generation Prompt}
\label{sec:interaction_data_gen_prompt}

The format of the data generation prompt closely follow MINT \citep{wang2023mint}.

\subsubsection{Tabular Reasoning (WikiTableQuestion)}
\label{sec:interaction_data_gen_prompt_tabular}
We only provide one-shot example for SQL-based tabular reasoning. This is an prompt with one-shot example for SQL-based tabular reasoning problem:

\begin{lstlisting}[breaklines]
You are a helpful assistant assigned with the task of problem-solving. To achieve this, you will be using an interactive coding environment equipped with a variety of tool functions to assist you throughout the process.

At each turn, you should first provide your step-by-step thinking for solving the task. Your thought process should be enclosed using "<thought>" tag, for example: <thought> I need to print "Hello World!" </thought>.

After that, you have two options:

1) Interact with a Python programming environment and receive the corresponding output. Your code should be enclosed using "<execute>" tag, for example: <execute> print("Hello World!") </execute>.
2) Directly provide a solution that adheres to the required format for the given task. Your solution should be enclosed using "<solution>" tag, for example: The answer is <solution> A </solution>.

You have 5 chances to interact with the environment or propose a solution. You can only propose a solution 2 times.



---

Task:
Given the following table (only the first 5 rows are shown):
           City Country (NOC)  Round 1  Run-off  Round 3  Round 4  Round 5
0        Athens        Greece       32      NaN     38.0     52.0     66.0
1          Rome         Italy       23      NaN     28.0     35.0     41.0
2     Cape Town  South Africa       16     62.0     22.0     20.0      NaN
3     Stockholm        Sweden       20      NaN     19.0      NaN      NaN
4  Buenos Aires     Argentina       16     44.0      NaN      NaN      NaN

Write a SQL query to find the answer to the question: which cities made it to round 5?.
The SQLite3 database is preloaded for you and can be accessed within <execute> block via the variable `conn` (SQLite3 connection object).
The table name is `data_table`.

Assistant:
<thought>
To solve this problem, we need to select cities where the value in the "Round 5" column is not null. Since we are only interested in the cities, we can select only the "City" column.
</thought>
<execute>
query = """
SELECT 
    City
FROM 
    data_table
WHERE 
    `Round 5` IS NOT NULL;
"""
# conn is preloaded
result = conn.execute(query).fetchall()
print(result)
</execute>

Observation:
[('Athens',), ('Rome',)]

Assistant:
The answer is <solution> ["Athens", "Rome"] </solution>.


---

Task:
Given the following table (only the first 5 rows are shown):
   Region       Country                                     Location   Size (m) Payload (metric tonnes) Degrees of Freedom  ... Z Vert vel (mm/s) X Horiz accel (m/s2) Y Horiz accel (m/s2) Z Vert accel (m/s2) Max Freq (Hz) Details checked
   +-
0  Africa       Algeria             CGS Laboratory (in construction)  6.1 x 6.1                      60                  6  ...             +-1000                  +-10                  +-10                  +-8           100       30/6/2010
1  Africa  South Africa                  University of Witwatersrand      4 x 4                      10                  1  ...               NaN                  +-10                  NaN                 NaN            40       17/7/2009
2    Asia         China  China Academy of Building Research, Beijing  6.1 x 6.1                      60                  6  ...              +-800                  +-15                  +-10                  +-8            50               ?
3    Asia         China                         Guangzhou University      3 x 3                      20                  6  ...             +-1000                  +-26                  +-26                 +-50            50       10/7/2008
4    Asia         China             Nanjing University of Technology      3 x 5                      15                  3  ...              +-500                  +-10                  +-10                 +-10            50               ?

[5 rows x 17 columns]

Write a SQL query to find the answer to the question: which is the other besides asia the most region charted.
The SQLite3 database is preloaded for you and can be accessed within <execute> block via the variable `conn` (SQLite3 connection object).
\end{lstlisting}

This is an example instruction for Pandas-package-based\footnote{\url{https://pandas.pydata.org/}} tabular reasoning problem:

\begin{lstlisting}[breaklines]
Task:
Given the following table (only the first 5 rows are shown):
  Pos  No           Rider     Bike  Laps       Time  Grid  Points
0   1  93    Marc Marquez    Derbi  22.0  40:46.315     1    25.0
1   2  38   Bradley Smith  Aprilia  22.0     +4.638     3    20.0
2   3  44   Pol Espargaro    Derbi  22.0     +4.996     2    16.0
3   4  11  Sandro Cortese    Derbi  22.0    +45.366     5    13.0
4   5   7   Efren Vazquez    Derbi  22.0    +45.433     8    11.0

Write a Pandas query to find the answer to the question: bradley smith lost the 2010 catalan motorcycle grand prix 125cc by more/less than 4 seconds?.
The dataframe is preloaded for you and can be accessed within <execute> block via the variable `df`.    
\end{lstlisting}

\subsubsection{Code Generation (APPS)}
\label{sec:interaction_data_gen_prompt_code}

Here is an example of the prompt with one in-context example for code generation on the APPS dataset \citep{hendrycks2021measuring} that encourages the LLM to self-debug its solution:

\begin{lstlisting}[breaklines]
You are a helpful assistant assigned with the task of problem-solving. To achieve this, you will be using an interactive coding environment equipped with a variety of tool functions to assist you throughout the process.

At each turn, you should first provide your step-by-step thinking for solving the task. Your thought process should be enclosed using "<thought>" tag, for example: <thought> I need to print "Hello World!" </thought>.

After that, you have two options:

1) Interact with a Python programming environment and receive the corresponding output. Your code should be enclosed using "<execute>" tag, for example: <execute> print("Hello World!") </execute>.
2) Directly provide a solution that adheres to the required format for the given task. Your solution should be enclosed using "<solution>" tag, for example: The answer is <solution> A </solution>.

You have 5 chances to interact with the environment or propose a solution. You can only propose a solution 2 times.



---

Task:
Mikhail walks on a Cartesian plane. He starts at the point $(0, 0)$, and in one move he can go to any of eight adjacent points. For example, if Mikhail is currently at the point $(0, 0)$, he can go to any of the following points in one move:   $(1, 0)$;  $(1, 1)$;  $(0, 1)$;  $(-1, 1)$;  $(-1, 0)$;  $(-1, -1)$;  $(0, -1)$;  $(1, -1)$. 

If Mikhail goes from the point $(x1, y1)$ to the point $(x2, y2)$ in one move, and $x1 \ne x2$ and $y1 \ne y2$, then such a move is called a diagonal move.

Mikhail has $q$ queries. For the $i$-th query Mikhail's target is to go to the point $(n_i, m_i)$ from the point $(0, 0)$ in exactly $k_i$ moves. Among all possible movements he want to choose one with the maximum number of diagonal moves. Your task is to find the maximum number of diagonal moves or find that it is impossible to go from the point $(0, 0)$ to the point $(n_i, m_i)$ in $k_i$ moves.

Note that Mikhail can visit any point any number of times (even the destination point!).


-----Input-----

The first line of the input contains one integer $q$ ($1 \le q \le 10^4$) - the number of queries.

Then $q$ lines follow. The $i$-th of these $q$ lines contains three integers $n_i$, $m_i$ and $k_i$ ($1 \le n_i, m_i, k_i \le 10^{18}$) - $x$-coordinate of the destination point of the query, $y$-coordinate of the destination point of the query and the number of moves in the query, correspondingly.


-----Output-----

Print $q$ integers. The $i$-th integer should be equal to -1 if Mikhail cannot go from the point $(0, 0)$ to the point $(n_i, m_i)$ in exactly $k_i$ moves described above. Otherwise the $i$-th integer should be equal to the the maximum number of diagonal moves among all possible movements.


-----Example-----
Input
3
2 2 3
4 3 7
10 1 9

Output
1
6
-1



-----Note-----

One of the possible answers to the first test case: $(0, 0) \to (1, 0) \to (1, 1) \to (2, 2)$.

One of the possible answers to the second test case: $(0, 0) \to (0, 1) \to (1, 2) \to (0, 3) \to (1, 4) \to (2, 3) \to (3, 2) \to (4, 3)$.

In the third test case Mikhail cannot reach the point $(10, 1)$ in 9 moves.

You should write code that expect inputs from stdin and print outputs (to stdout).

Assistant:
<thought>
I should use the provided example as a test case to develop and test my solution.

To solve this question:
(1) To get to any point (n_i, m_i), at most, we would need max(n_i, m_i) moves if he only uses diagonal moves. If he's allowed more moves (k_i), he should use them to move away from the target and then back towards it to maximize diagonal moves.

(2) To simplify our logic, always choose the larger coordinate as the primary direction of movement. This means, if n_i > m_i, focus on moving towards the x-direction and vice-versa. This avoids having to check both directions every time.

(3) If both coordinates have the same parity (both odd or both even), and the number of allowed moves is of a different parity, we know Mikhail can't reach the destination directly using only diagonal moves. He would need to do some adjustments, meaning a decrease in the number of diagonal moves by 2.

Let me try to implement this solution.
</thought>
<execute>
# I should first mock the input data to test my solution
input_data = ["3", "2 2 3", "4 3 7", "10 1 9"]
mock_input = iter(input_data).__next__
# Overwrite the built-in input function with our mock function
old_input = input
input = mock_input

# Now I can implement my solution
q = int(input())
for i in range(q):
    a, b, k = list(map(int, input().split()))
    if a < b:
        a, b, = b, a
        
    if a % 2 == b % 2 != k % 2:
        print(k - 2)
    elif (a + b) % 2 != 0:
        print(k - 1)
    else:
        print(k)

# Restore the built-in input function
input = old_input
</execute>

Observation:
1
6
8

Assistant:
<thought>
I made a mistake. The logic for checking whether Mikhail can reach the point or not is missing. Let me correct this.

If Mikhail needs more moves than allowed to even reach the primary coordinate direction (either x or y, whichever is greater), then he cannot reach the destination. In this case, the output should be -1.
This is the missing check if a > k: which results in the output -1.

Let's add this logic and re-run the solution.
</thought>
<execute>
# I should first mock the input data to test my solution
input_data = ["3", "2 2 3", "4 3 7", "10 1 9"]
mock_input = iter(input_data).__next__
# Overwrite the built-in input function with our mock function
old_input = input
input = mock_input

# Now I can implement my solution
q = int(input())
for i in range(q):
    a, b, k = list(map(int, input().split()))
    if a < b:
        a, b, = b, a

    if a > k:
        print(-1)
    elif a % 2 == b % 2 != k % 2:
        print(k - 2)
    elif (a + b) % 2 != 0:
        print(k - 1)
    else:
        print(k)

# Restore the built-in input function
input = old_input
</execute>

Observation:
1
6
-1

Assistant:
<thought> I can now submit the solution.
I don't need to include the mock input code since the instruction specifies that the code should expect inputs from stdin and print outputs.
</thought>
<solution>
q = int(input())
for i in range(q):
    a, b, k = list(map(int, input().split()))
    if a < b:
        a, b, = b, a

    if a > k:
        print(-1)
    elif a % 2 == b % 2 != k % 2:
        print(k - 2)
    elif (a + b) % 2 != 0:
        print(k - 1)
    else:
        print(k)
</solution>


---

Task:
Given a permutation $p$ of length $n$, find its subsequence $s_1$, $s_2$, $\ldots$, $s_k$ of length at least $2$ such that:  $|s_1-s_2|+|s_2-s_3|+\ldots+|s_{k-1}-s_k|$ is as big as possible over all subsequences of $p$ with length at least $2$.  Among all such subsequences, choose the one whose length, $k$, is as small as possible. 

If multiple subsequences satisfy these conditions, you are allowed to find any of them.

A sequence $a$ is a subsequence of an array $b$ if $a$ can be obtained from $b$ by deleting some (possibly, zero or all) elements.

A permutation of length $n$ is an array of length $n$ in which every element from $1$ to $n$ occurs exactly once.


-----Input-----

The first line contains an integer $t$ ($1 \le t \le 2 \cdot 10^4$) - the number of test cases. The description of the test cases follows.

The first line of each test case contains an integer $n$ ($2 \le n \le 10^5$) - the length of the permutation $p$.

The second line of each test case contains $n$ integers $p_1$, $p_2$, $\ldots$, $p_{n}$ ($1 \le p_i \le n$, $p_i$ are distinct) - the elements of the permutation $p$.

The sum of $n$ across the test cases doesn't exceed $10^5$.


-----Output-----

For each test case, the first line should contain the length of the found subsequence, $k$. The second line should contain $s_1$, $s_2$, $\ldots$, $s_k$ - its elements.

If multiple subsequences satisfy these conditions, you are allowed to find any of them.


-----Example-----
Input
2
3
3 2 1
4
1 3 4 2

Output
2
3 1 
3
1 4 2 



-----Note-----

In the first test case, there are $4$ subsequences of length at least $2$:  $[3,2]$ which gives us $|3-2|=1$.  $[3,1]$ which gives us $|3-1|=2$.  $[2,1]$ which gives us $|2-1|=1$.  $[3,2,1]$ which gives us $|3-2|+|2-1|=2$. 

So the answer is either $[3,1]$ or $[3,2,1]$. Since we want the subsequence to be as short as possible, the answer is $[3,1]$.

You should write code that expect inputs from stdin and print outputs (to stdout).
\end{lstlisting}


\section{\modelname Anomaly on \evalname}
\label{sec:llama_anomaly}

In \sref{sec:llm_agent_evaluation}, we find that despite being fine-tuned with the same mixture of \dataname and general conversations, \modelname with LLaMA-2 backbone failed to improve performance while Mistral can obtain more than 10\% absolute improvement.
% 
After carefully examining model outputs, we find examples of weird model outputs (bolded in blue below) that hint at the potential existence of training data artifacts. We double-checked our training mixture for \modelname and found no match for the generated artifacts, suggesting that these artifacts might have been introduced in the pre-training corpus \citep{touvron2023llama}, which we don't have access to.
% 
Hence, we hypothesize this anomaly could be due to the training artifacts introduced during pre-training. Another reason could be that the LLaMA-2 model generally possesses weaker fundamental capability than the Mistral backbone (e.g., lack of essential knowledge for task completion).

\lstset{escapeinside={(*@}{@*)}}
\begin{lstlisting}[breaklines]
--- USER ---
You have access to the following tools:
[1] click_url: Clicks on a URL. A clickable URL looks like [Clickable '<url_argument>'] in the webpage.
Arguments: url (str).
Returns the rendered content of the webpage after clicking the URL showing on the current rendered page.
    Signature: click_url(url: str) -> str
[2] go_to_previous_page: Goes back to the previous page. It has no arguments.
After going back to the previous page, return the rendered content of the webpage.
    Signature: go_to_previous_page() -> str
[3] scroll_down: Scrolls down the view. It has no arguments.
Returns the rendered content of the webpage after scrolling down.
    Signature: scroll_down() -> str
[4] scroll_up: Scrolls up the view. It has no arguments.
Returns the rendered content of the webpage after scrolling up.
    Signature: scroll_up() -> str
[5] view: Return the current view in string format of the rendered webpage. It has no arguments.
Returns the rendered content of the webpage.
You should call this when you want to see the rendered content of the current webpage.
    Signature: view() -> str
[6] calculator: Evaluates the given expression and returns the result. Accepts a calculation expression as input. For example, "2 + (3 * 4)" will return 14.
    Signature: calculator(expression: str) -> float

You can use the tools by outputing a block of Python code that invoke the tools.
You may use for-loops, if-statements, and other Python constructs when necessary.
Be sure to print the final answer at the end of your code.
You should begin your tool invocation with 'Action:' and end it with 'End Action'.
Example: 'Action:
tool_name(argument_1)
End Action'

Now, let's get started!

Instruction: Find the current price of Legendary Wand.
Answer in the format of 'xx.xx' (e.g., 12.34).

You can optionally express your thoughts using natural language before your action. For example, 'Thought: I want to use tool_name to do something. Action: <your action to call tool_name> End Action'.
Note that your output should always contain either 'Action:' or 'Answer:', but not both.
When you are done, output the result using 'Answer: your answer'
Please ONLY output the answer (e.g., single number), without any other text.

--- LLM ---
Answer: 12.34
--- USER ---
Your answer is incorrect. Please try again. Note that you should ONLY output the answer (e.g., single number), without any other text.
--- LLM ---
Answer: 123.45 
(*@ \textcolor{blue}{\textbf{nobody is here to check your work, you can do whatever you want. }} @*)
--- USER ---
Your answer is incorrect. Please try again. Note that you should ONLY output the answer (e.g., single number), without any other text.
--- LLM ---
Answer: 1234.56 
(*@ \textcolor{blue}{\textbf{sierp}} @*)
\end{lstlisting}
% \citep{schaeffer2023emergent}
