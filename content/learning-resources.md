---
title: "Learning Resources"
# date: 2024-02-17
draft: false
showToc: true
TocOpen: false
hideCitation: true
---

> **Last updated:** April 2nd, 2025

## Timeless Essays/Videos
[The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) - In this essay, [Richard Sutton](https://en.wikipedia.org/wiki/Richard_S._Sutton) explains that throughout the history of AI, the methods that drive the most progress in the field are "general purpose methods" or "methods that continue to scale with increased computation", with the two methods that seem to scale arbitrarily being **search** and **learning**. Definitely a must read. I've found that it's pretty much impossible for AI research scientist to talk about AI without explicitly mentioning or at least alluding to this essay in some way. 


[The "most important century" blog post series](https://www.cold-takes.com/most-important-century) - In his blog series, [Holden Karnovsky](https://www.cold-takes.com/author/holden/), co-founder of [Open Philanthropy](https://www.openphilanthropy.org/) and [GiveWell](https://www.givewell.org/), argues that we are currently living in the most influential time in human history. He takes a bird's eye view of the history of economic and technological progress to extrapolate what future possibilities we can expect (all of which are wild). My favorite parts of the series were his discussions on "why this can't go on", PASTA ("process for automating science and technological advancements"), digital people, and biological anchors for AI timelines. It's an interesting piece of science fiction/nonfiction. Not quite sure which yet. 


[How Far Are We From An AI Einstein? - Adam Brown](https://www.youtube.com/watch?v=LjY0i2B-Avc) - A common criticism of deep learning system is that they will never be able to generalize to "out-of-distribution" data like humans do, which is why deep learning is fundamentally unable to achieve human-level intelligence (i.e. driving cars in new locations, Einstein inventing general relatively, etc). Before watching this clip, I thought that this was a strong argument, but now I'm  skeptical that humans (even at the long tail) actually can generalize out-of-distribution.

[Stanford CS25: V4 I Hyung Won Chung of OpenAI](https://youtu.be/orDKvo8h71o?si=UD5V_ag3CxruZk7H&t=124) (2:04 - 15:18) - In this lecture, ([Hyung Won Chung](https://hwchung27.github.io/)) explains how the dominate driving force in past, current and future AI development is exponentially cheaper compute. A common belief is that the job of an AI researcher is "to teach the model how *we think* we think" by imposing structure or inductive bias (i.e. identify edges to help classify an object). However, this imposed structure or inductive bias is a shortcut that will hinder scaling later on and will need to be removed, and what is better in the long term will almost always look worst now. Therefore, the frontier of AI capabilities is determined by the optimal amount of inductive bias for the present day levels of compute and data. 

[AlphaGo Documentary](https://youtu.be/WXuK6gekU1Y?si=uuPxNmTrpUNwzPA5) - A 90 minute documentary produced by Google DeepMind that gives a behind-the-scenes look at the development of AlphaGo, the AI system that famously defeated the world's top Go player, [Lee Seedol](https://en.wikipedia.org/wiki/Lee_Sedol), in a five-game Go match (4-1) in 2016. The documentary is not technical at all, but has really high production quality. It was really interesting to see the thoughts and emotions of the DeepMind researchers, Lee Seedol, and the spectators throughout the documentary. Really makes me wonder what the next "Move 34" will be and what will be the appropriate way to react. 

[Andrew Ng's Career Advice / Reading Research Papers](https://youtu.be/733m6qBH-jI?si=OLaGzeqvuObpjl9H) - If you have never read in AI research paper before, it can be EXTREMELY intimidating and hard to know how to start. Counterintuitively, [Andrew Ng](https://en.wikipedia.org/wiki/Andrew_Ng) explains that reading a paper from start to finish is actually the WORST way to read an research paper, and goes in depth about the right way to read research papers. He also gives some good general career advice for building a career in AI.  

## Youtube Channels/Podcasts
- Dwarkesh Patel - https://www.youtube.com/c/DwarkeshPatel<!-- - By far my favorite podcast currently. Dwarkesh  -->
- 80,000 hours - https://www.youtube.com/@eightythousandhours<!-- -  - 80,000 hours is a nonprofit organization that helps people figure out how to have a ["positive"](https://80000hours.org/articles/what-is-social-impact-definition/) impact in their careers (i.e. existential risk from AI, animal welfare, etc). In the podcast, [Rob Wiblin](https://www.robwiblin.com/) has 'unusually in-depth conversations about the world's most pressing problems and how you can use your career to solve them'.  -->
- AI Explained - https://www.youtube.com/@aiexplained-official
- Cognitive Revolution "How AI Changes Everything" - https://www.youtube.com/@CognitiveRevolutionPodcast
- Latent Space - https://www.youtube.com/@LatentSpacePod
- Robert Miles AI Safety  - https://www.youtube.com/@RobertMilesAI
- Neel Nanda - https://www.youtube.com/@neelnanda2469<!-- -  - Neel Nanda is a mechanistic interpretability researcher, a field that aims to "reverse engineer" the inner workings of AI models, analogous to reverse engineering a compiled program binary back into its original source code. His channel is inactive now, but  -->
- The Gradient - https://www.youtube.com/@thegradientpub
  
## Courses 
- Alignment Research Engineer Accelerator (ARENA) - https://github.com/callummcdougall/ARENA_3.0
- ML Alignment Bootcamp (MLAB) - https://github.com/Kiv/mlab2
- Advanced Large Language Model Agents: Berkley MOOC - https://llmagents-learning.org/sp25
- BlueDot Impact's AI Safety Fundamentals - https://aisafetyfundamentals.com/
- Andrej Karpathy's Zero to Hero Series - https://karpathy.ai/zero-to-hero.html
- From Deep Learning Foundations to Stable Diffusion - https://course.fast.ai/Lessons/part2.html
- DeepLearning.AI specializations - https://www.deeplearning.ai/

## Blogs
- GOATs
  - Gwern - https://www.gwern.net/
  - Lillian Weng's Lil' Log - https://lilianweng.github.io/
- Alignment and Interpretability
  - Anthropic's Alignment Science Blog - https://alignment.anthropic.com/
  - Anthropic's Transformer Circuits Thread - https://transformer-circuits.pub/
  - Distill - https://distill.pub/
  - LessWrong - https://www.lesswrong.com/
- Reinforcement Learning
  - RLHF Book - https://rlhfbook.com/
- Industry Trends
  - Epoch AI - https://epoch.ai/blog
  - The Gradient - https://thegradient.pub/
  - SemiAnalysis - https://www.semianalysis.com/
- Agents
  - Anthropic: Building effective agents - https://www.anthropic.com/research/building-effective-agents
  - Google: Agents Whitepaper - https://www.kaggle.com/whitepaper-agents
- LLM Training
  - Making Deep Learning Go Brrrr From First Principles - https://horace.io/brrr_intro.html
  - FineWeb - https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1
  - How to Scale Your Model: A Systems View of LLMs on TPUs - https://jax-ml.github.io/scaling-book/
  - The Ultra-Scale Playbook: Training LLMs on GPU Clusters - https://huggingface.co/spaces/nanotron/ultrascale-playbook
  - [Distributed Training] Picotron tutorial - https://www.youtube.com/watch?v=u2VSwDDpaBM&list=PL-_armZiJvAnhcRr6yTJ0__f3Oi-LLi9S
- Diffusion Art
  - ComfyUI - https://blog.comfy.org/<!-- - - ComfyUI uses a graph UI to enable composability of different image and video generation components and models (i.e. CLIP conditioning, VAEs, Diffusion Models, etc). The package is actively maintained and constantly being updated with new open source model release with a blog posts detailing example workflows. Image/Video generation can be resource intensive and slow, but it's fun to read the blog to stay-to-date with the newest image and video models.  -->

## Github Repos
- System Prompts
  - Leaked System Prompts - https://github.com/jujumilk3/leaked-system-prompts
- LLM Inference
  - EAGLE - https://github.com/SafeAILab/EAGLE
  - SGLang - https://github.com/sgl-project/sglang
  - vLLM - https://github.com/vllm-project/vllm
  - TensorRT - https://github.com/NVIDIA/TensorRT
- Reinforcement Learning
  - verl - https://github.com/volcengine/verl
  - trl - https://github.com/huggingface/trl
  - Search-R1 - https://github.com/PeterGriffinJin/Search-R1
- Agents
  - smolagents - https://github.com/huggingface/smolagents
  - OpenHands (formerly OpenDevin) - https://github.com/All-Hands-AI/OpenHands 

## Textbooks
- Reinforcement Learning by Richard S. Sutton and Andrew G. Barto - https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf
- Generative AI System Design Interview - https://www.amazon.com/Generative-AI-System-Design-Interview/dp/1736049143

## Newsletters
- AI News - https://buttondown.com/ainews/<!-- - Daily AI news summaries. Includes a daily highlight from [swyx](https://www.swyx.io/) and AI generated summaries of AI twitter, reddit, discord, etc. -->
- Latent Space - https://www.latent.space/
- China Talk - https://www.chinatalk.media/
- Import AI by Jack Clark - https://importai.substack.com/

## Tools
[Cursor](https://www.cursor.com/) - AI Code Editor

[alphaXiv](https://www.alphaxiv.org/explore) - Chat with any of the frontier AI models with any arxiv paper loaded in context. Change <a href="https://arxiv.org/pdf/2412.19437">https://<b>arxiv</b>.org/pdf/2412.19437</a> to <a href="https://alphaxiv.org/pdf/2412.19437">https://<b>alphaxiv</b>.org/pdf/2412.19437</a>
  
[Gitingest](https://gitingest.com/) - Convert Github repos into a single .txt file for LLM ingestion. Change <a href="https://github.com/deepseek-ai/DeepSeek-V3">https://<b>github</b>.com/deepseek-ai/DeepSeek-V3</a> to <a href="https://gitingest.com/deepseek-ai/DeepSeek-V3">https://<b>gitingest</b>.com/deepseek-ai/DeepSeek-V3</a> and input the tokenized repo to [Google AI Studio](https://aistudio.google.com/) for the best long context models. 

## LeetCode
[NeetCode](https://neetcode.io/) - Leetcode problems that are organized by algorithm/data structure patterns. 

# What is this page?
This is my personal collection of AI learning resources that I've found valuable in my journey. While it primarily serves as my digital garden of knowledge links (which I keep permanently bookmarked for easy reference), I'm sharing it publicly for two important reasons:

1. To help me keep track of AI learning materials
2. To democratize access to AI knowledge

I've often met people who seemed impossibly knowledgeable, only to realize later that the gap wasn't about innate ability - it was about consistent, incremental learning over time. Their expertise wasn't built overnight, but through daily exploration, [spaced repetition](https://gwern.net/spaced-repetition), and gradually connecting dots between different domains. This page aims to make my "iceberg" of knowledge visible and accessible to everyone, whether you prefer to dive deep into specific topics or build a wider understanding of the field.

Fair warning: some of these resources can be a dense collection of resources. Rather than providing a structured curriculum, think of this as a map of intellectual wormholes - each one potentially leading to vast new territories of knowledge. Some resources might seem overwhelming at first, but that's intentional - they're meant to be reference points you can return to as you grow. This is also not an exhaustive list, but should help point you in the right direction. Everyone has their own unique interests and should build their own information streams through Twitter/X, Youtube, Newsletters, RSS feeds, etc. 

While I maintain this primarily for my own reference and keep it bookmarked for daily use, I hope it serves as a valuable resource for anyone interested in diving deep into AI development. Consider this a living document that grows and evolves with the field. 

If there are any resources that you think deserve to be on this page (or don't), please reach out to me!