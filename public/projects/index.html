<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=55382&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Projects | ML&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="GPT-2 Transformer Implementation and Mechanistic Interpretability Experimentation

A comprehensive implementation and exploration of transformer-based language models, focusing on GPT-2 architecture and mechanistic interpretability. This project features three main components:

a production-ready lightweight implementation with efficient training pipelines based on Andrej Karpathy&rsquo;s nanoGPT
a clean, educational GPT-2 implementation from scratch with detailed documentation and intuition (includes sampling techniques with temperature, top-k, top-p, beam search and KV caching)
advanced mechanistic interpretability tools for analyzing model internals including attention patterns, induction heads, feature representations, circuit behavior, and toy models of superposition

Built with PyTorch, TransformerLens, and integrated with Weights &amp; Biases for experiment tracking.">
<meta name="author" content="Michael Liu">
<link rel="canonical" href="http://localhost:55382/projects/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.ae03dc1363b0d274c8a5b8f4ef4b43ec376146d505cc14962ea16577e875c413.css" integrity="sha256-rgPcE2Ow0nTIpbj070tD7DdhRtUFzBSWLqFld&#43;h1xBM=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:55382/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:55382/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:55382/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:55382/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:55382/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:55382/projects/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$']],
    displayMath: [['\\[', '\\]'], ['$$', '$$']],
    processEscapes: true,
    processEnvironments: true
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  },
  startup: {
    pageReady: () => {
      return MathJax.startup.defaultPageReady().then(() => {
        console.log('MathJax initial typesetting complete');
      });
    }
  },
  chtml: {
    fontURL: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2'
  }
};
</script>
<script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

    <link rel="stylesheet" href="/css/custom.css">
    <script src="/js/zoom.js"></script> 
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:55382/" accesskey="h" title="ML&#39;s Blog (Alt + H)">ML&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:55382/posts/" title="Posts">
                    <span>Posts</span>
                    
                </a>
            </li>
            <li>
                <a href="http://localhost:55382/projects/" title="Projects">
                    <span class="active">Projects</span>
                    
                </a>
            </li>
            <li>
                <a href="http://localhost:55382/learning-resources/" title="Learning Resources">
                    <span>Learning Resources</span>
                    
                </a>
            </li>
            <li>
                <a href="http://localhost:55382/about/" title="About">
                    <span>About</span>
                    
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:55382/">Home</a></div>
    <h1 class="post-title entry-hint-parent">
      Projects
    </h1>
    <div class="post-meta">9 min&nbsp;Â·&nbsp;Michael Liu

</div>
  </header> <aside id="toc-container" class="toc-container wide">
<div class="toc">
    <details id="toc-details">
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#gpt-2-transformer-implementation-and-mechanistic-interpretability-experimentation" aria-label="GPT-2 Transformer Implementation and Mechanistic Interpretability Experimentation">GPT-2 Transformer Implementation and Mechanistic Interpretability Experimentation</a></li>
                <li>
                    <a href="#reinforcement-learning-implementations" aria-label="Reinforcement Learning Implementations">Reinforcement Learning Implementations</a></li>
                <li>
                    <a href="#llm-evaluation-and-agent-framework" aria-label="LLM Evaluation and Agent Framework">LLM Evaluation and Agent Framework</a></li>
                <li>
                    <a href="#diffusion-and-multimodal-models-implementation" aria-label="Diffusion and Multimodal Models Implementation">Diffusion and Multimodal Models Implementation</a>
                </li>
            </ul>
        </div>
    </details>
</div>
</aside>
<script>
    let activeElement;
    let elements;
    let tocDetails;
    let tocContainer;
    
    window.addEventListener('DOMContentLoaded', function (event) {
        tocContainer = document.getElementById("toc-container");
        tocDetails = document.getElementById('toc-details');
        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        if (!tocDetails || !tocContainer) return;

        checkTocPosition();
        
        if (elements.length > 0) {
            activeElement = elements[0];
            const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
            const activeLink = document.querySelector(`.inner ul li a[href="#${id}"]`);
            if (activeLink) {
                activeLink.classList.add('active');
            }
        }
    });

    window.addEventListener('resize', checkTocPosition);

    window.addEventListener('scroll', () => {
        if (!elements || elements.length === 0) return;

        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement;

        elements.forEach(element => {
            const id = encodeURI(element.getAttribute('id')).toLowerCase();
            const link = document.querySelector(`.inner ul li a[href="#${id}"]`);
            if (link) {
                if (element === activeElement) {
                    link.classList.add('active');
                } else {
                    link.classList.remove('active');
                }
            }
        });
    }, { passive: true });

    function checkTocPosition() {
        if (!tocDetails || !tocContainer) return;

        const width = document.body.scrollWidth;
        const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
        const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
        const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            tocContainer.classList.add("wide");
            tocDetails.setAttribute('open', '');
        } else {
            tocContainer.classList.remove("wide");
            tocDetails.removeAttribute('open');
        }
    }

    function getOffsetTop(element) {
        if (!element || !element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><h2 id="gpt-2-transformer-implementation-and-mechanistic-interpretability-experimentation">GPT-2 Transformer Implementation and Mechanistic Interpretability Experimentation<a hidden class="anchor" aria-hidden="true" href="#gpt-2-transformer-implementation-and-mechanistic-interpretability-experimentation">#</a></h2>
<p><img alt="GPT-2 Transformer" loading="lazy" src="/assets/images/gpt2-transformer-image.png"></p>
<p>A comprehensive implementation and exploration of transformer-based language models, focusing on GPT-2 architecture and mechanistic interpretability. This project features three main components:</p>
<ul>
<li>a production-ready lightweight implementation with efficient training pipelines based on Andrej Karpathy&rsquo;s <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a></li>
<li>a clean, educational GPT-2 implementation from scratch with detailed documentation and intuition (includes sampling techniques with temperature, top-k, top-p, beam search and KV caching)</li>
<li>advanced mechanistic interpretability tools for analyzing model internals including attention patterns, induction heads, feature representations, circuit behavior, and toy models of superposition</li>
</ul>
<p>Built with PyTorch, <a href="https://github.com/TransformerLensOrg/TransformerLens">TransformerLens</a>, and integrated with Weights &amp; Biases for experiment tracking.</p>
<p><a href="https://github.com/michaelyliu6/gpt2-transformer">View Project on GitHub</a></p>
<details>
<summary>Image Prompt</summary>
<i>An anime-style visualization of a transformer architecture laboratory. In the foreground, a character with digital glasses is analyzing a glowing, multi-layered neural network structure. The central feature is an exploded view of a transformer block with attention heads visualized as colorful beams connecting token representations. Each attention head is depicted as a unique anime-style entity with its own personality, examining different aspects of the input text. The scene shows multiple screens displaying attention patterns, with one large display showing how different heads attend to different parts of a sentence. Another screen visualizes the internal representations of words transforming as they pass through each layer. The laboratory features circuit diagrams floating in holographic displays, showing the flow of information through the model with particular emphasis on induction heads and trigram detection circuits. In the background, several smaller anime characters represent different components of the architecture: embedding lookup tables, feed-forward networks, and layer normalization. The entire scene is bathed in a blue-green digital glow, with streams of token embeddings flowing between components. Mathematical equations for attention mechanisms and layer transformations are elegantly integrated into the scene's design elements. The visualization combines technical accuracy with an artistic anime aesthetic, making the complex architecture both beautiful and comprehensible.</i>
<br><br>
</details>
<details>
<summary>References</summary>
- ARENA Chapter 1: Transformer Interpretability - <a href="https://arena-chapter1-transformer-interp.streamlit.app/">https://arena-chapter1-transformer-interp.streamlit.app/</a><br>
- Attention Is All You Need - <a href="https://arxiv.org/pdf/1706.03762">https://arxiv.org/pdf/1706.03762</a><br>
- Language Models are Unsupervised Multitask Learners (GPT-2) - <a href="https://arxiv.org/pdf/2005.14165">https://arxiv.org/pdf/2005.14165</a><br>
- Language Models are Few-Shot Learners (GPT-3) - <a href="https://arxiv.org/pdf/2005.14165">https://arxiv.org/pdf/2005.14165</a><br>
- What is a Transformer? (Transformer Walkthrough Part 1/2) - <a href="https://youtu.be/bOYE6E8JrtU?si=aZ2KFIXRjOyxWr52">https://youtu.be/bOYE6E8JrtU?si=aZ2KFIXRjOyxWr52</a><br>
- A Mathematical Framework for Transformer Circuits - <a href="https://transformer-circuits.pub/2021/framework/index.html">https://transformer-circuits.pub/2021/framework/index.html</a><br>
- An Analogy for Understanding Transformers - <a href="https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-analogy-for-understanding-transformers">https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-analogy-for-understanding-transformers</a><br>
- Induction heads - illustrated - <a href="https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated">https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated</a><br>
- Transformer Feed-Forward Layers Are Key-Value Memories - <a href="https://arxiv.org/pdf/2012.14913">https://arxiv.org/pdf/2012.14913</a><br>
- Toy Models of Superposition - <a href="https://transformer-circuits.pub/2022/toy_model/index.html">https://transformer-circuits.pub/2022/toy_model/index.html</a>
</details>
<h2 id="reinforcement-learning-implementations">Reinforcement Learning Implementations<a hidden class="anchor" aria-hidden="true" href="#reinforcement-learning-implementations">#</a></h2>
<p><img alt="Reinforcement Learning" loading="lazy" src="/assets/images/rl-project-image.png"></p>
<p>This project implements a range of RL techniques, culminating in Reinforcement Learning from Human Feedback (RLHF) for transformer language models. It demonstrates a progression from basic RL (multi-armed bandits, Q-learning) to advanced methods (DQN, PPO, and RLHF). This project features four main components:</p>
<ul>
<li>Core RL concepts by implementing classic bandit algorithms (Epsilon-Greedy, UCB, etc.) using <code>gymnasium</code>.</li>
<li>Model-free RL with Q-Learning, SARSA, and Deep Q-Networks (DQN), including a replay buffer. Uses CartPole and probe environments.</li>
<li>Policy-gradient algorithm, actor-critic architecture, GAE, and a clipped surrogate objective.  Applies to CartPole (and Atari work in progress).</li>
<li>Applying RL to train transformer language models using human feedback (simulated).  Includes a value head, reward function, KL divergence penalty, and a complete RLHF training loop using <code>transformer_lens</code>.</li>
</ul>
<p>Built with PyTorch, OpenAI Gym, and integrated with Weights &amp; Biases for experiment tracking.</p>
<p><a href="https://github.com/michaelyliu6/reinforcement-learning">View Project on GitHub</a></p>
<details>
<summary>Image Prompt</summary>
<i>An anime-style scene depicting a group of cute robot characters in a world made of classic Atari game elements. In the foreground, an excited robot with glowing eyes and animated facial expressions has just successfully navigated through a Pac-Man-style maze filled with colorful dots and ghosts. The robot stands triumphantly at the maze exit, surrounded by sparkling reward particles and a floating '10000 POINTS' text in retro pixelated font. Behind it, the conquered maze shows its successful path highlighted in glowing light. From the successful robot's core, streams of colorful data and code are flowing back to three other robot characters waiting at different Atari-inspired challenges: one facing a wall of Space Invaders aliens, another preparing to bounce a Breakout ball with a paddle, and a third positioned before a Pong game setup. Each watching robot has holographic displays showing the successful algorithm and strategy being shared. All robots have distinct anime designs with expressive digital eyes, sleek bodies with retro gaming color schemes (reds, blues, yellows), and cute proportions. The background features a pixelated landscape with more Atari game elements including Adventure dragons and Asteroids space rocks. The scene is rendered in vibrant anime style with clean lines, digital effects, and the characteristic glow of arcade screens illuminating the robots' metallic surfaces.</i> - Generated by Flux 1.1 Pro
<br><br>
</details>
<details>
<summary>References</summary>
- ARENA Chapter 2: Reinforcement Learning - <a href="https://arena-chapter2-rl.streamlit.app/">https://arena-chapter2-rl.streamlit.app/</a><br>
- Reinforcement Learning by Richard S. Sutton and Andrew G. Barto - <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf</a><br>
- Q-Learning - <a href="https://link.springer.com/content/pdf/10.1007/BF00992698.pdf">https://link.springer.com/content/pdf/10.1007/BF00992698.pdf</a><br>
- Playing Atari with Deep Reinforcement Learning - <a href="https://arxiv.org/pdf/1312.5602">https://arxiv.org/pdf/1312.5602</a><br>
- An introduction to Policy Gradient methods - Deep Reinforcement Learning - <a href="https://www.youtube.com/watch?v=5P7I-xPq8u8">https://www.youtube.com/watch?v=5P7I-xPq8u8</a><br>
- Proximal Policy Optimization Algorithms - <a href="https://arxiv.org/pdf/1707.06347">https://arxiv.org/pdf/1707.06347</a><br>
- The 37 Implementation Details of Proximal Policy Optimization - <a href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/">https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/</a><br>
- Deep Reinforcement Learning from Human Preferences - <a href="https://arxiv.org/pdf/1706.03741">https://arxiv.org/pdf/1706.03741</a><br>
- Illustrating Reinforcement Learning from Human Feedback (RLHF) - <a href="https://huggingface.co/blog/rlhf">https://huggingface.co/blog/rlhf</a><br>
- Training language models to follow instructions with human feedback - <a href="https://arxiv.org/pdf/2203.02155">https://arxiv.org/pdf/2203.02155</a>
</details>
<hr>
<h2 id="llm-evaluation-and-agent-framework">LLM Evaluation and Agent Framework<a hidden class="anchor" aria-hidden="true" href="#llm-evaluation-and-agent-framework">#</a></h2>
<p><img alt="LLM Evaluation" loading="lazy" src="/assets/images/llm-eval-image.png"></p>
<p>This project investigates the emergent behaviors of Large Language Models (LLMs) through a rigorous evaluation framework, emphasizing AI safety and alignment. The work encompasses the development and assessment of LLM-based agents, the automated generation of adversarial test cases, and the application of the <a href="https://inspect.ai-safety-institute.org.uk/"><code>inspect</code></a> library for structured evaluation. Key areas of investigation include:</p>
<ul>
<li><strong>Alignment Faking:</strong> Replication of findings from <a href="https://arxiv.org/pdf/2412.14093">Alignment Faking in Large Language Models</a>, demonstrating the potential for deceptive behavior in LLMs under varying deployment contexts, highlighting critical vulnerabilities in LLM deployment strategies.</li>
<li><strong>Dataset Generation via Meta-Evaluation:</strong> Threat Model design and automated generation of multiple-choice question datasets using LLMs, employing techniques such as few-shot prompting and iterative refinement. Concurrency is leveraged via <code>ThreadPoolExecutor</code> for efficient dataset creation.</li>
<li><strong>Structured Evaluation with <code>inspect</code>:</strong> Utilization of the <code>inspect</code> library to conduct systematic evaluations. This includes defining custom <code>solvers</code> to manipulate model inputs, <code>scorers</code> to quantify model outputs, and <code>tasks</code> to orchestrate the evaluation pipeline.</li>
<li><strong>Agent-Based Evaluation:</strong> Construction of autonomous agents leveraging LLM APIs, including the implementation of function calling for tool interaction. A &ldquo;WikiGame&rdquo; agent is developed using elicitation methods such as ReAct, Reflexion, etc, and evaluated on failure modes.</li>
</ul>
<p><a href="https://github.com/michaelyliu6/llm-evals">View Project on GitHub</a></p>
<details>
<summary>Image Prompt</summary>
<i>An anime-style scene showcasing a recursive AI evaluation laboratory. In the foreground, a scientist character with glasses and a digital tablet is orchestrating a multi-layered evaluation system. The central feature is a striking "evaluation inception" visualization - a series of nested, glowing rings representing LLMs evaluating other LLMs. Each ring contains AI entities analyzing the output of inner-ring AIs, with data flowing between layers. One AI character is generating test cases, passing them to a second AI that's producing responses, while a third AI is scoring those responses with complex metrics floating around it. A fourth AI is analyzing those scores and refining the evaluation criteria, creating a perfect loop. Holographic displays show this recursive process with labels like "Meta-Evaluation Layer 3" and "Alignment Verification Loop." In the background, several agent robots navigate a Wikipedia-themed maze, but now they're being observed by evaluator robots taking notes on clipboards. The laboratory features fractal-like screens showing the same evaluation patterns repeating at different scales. Digital metrics flow between systems in colorful streams, with some screens showing "Evaluator Bias Analysis" and "Meta-Alignment Testing." The entire scene has a recursive aesthetic with evaluation processes visibly nested within each other, all rendered in vibrant anime style with expressive AI characters showing varying degrees of concentration as they evaluate their peers.</i> - Generated by Flux 1.1 Pro
<br><br>
</details>
<details>
<summary>References</summary>
- ARENA Chapter 3: LLM Evaluations - <a href="https://arena-chapter3-llm-evals.streamlit.app/">https://arena-chapter3-llm-evals.streamlit.app/</a><br>
- Alignment faking in large language models - <a href="https://arxiv.org/pdf/2412.14093">https://arxiv.org/pdf/2412.14093</a><br>
- Discovering Language Model Behaviors with Model-Written Evaluations - <a href="https://arxiv.org/pdf/2212.09251">https://arxiv.org/pdf/2212.09251</a><br>
- A starter guide for evals - <a href="https://www.alignmentforum.org/posts/2PiawPFJeyCQGcwXG/a-starter-guide-for-evals">https://www.alignmentforum.org/posts/2PiawPFJeyCQGcwXG/a-starter-guide-for-evals</a><br>
- LLM Powered Autonomous Agents - <a href="https://lilianweng.github.io/posts/2023-06-23-agent/">https://lilianweng.github.io/posts/2023-06-23-agent/</a><br>
- Evaluating Language-Model Agents on Realistic Autonomous Tasks - <a href="https://arxiv.org/pdf/2312.11671">https://arxiv.org/pdf/2312.11671</a><br>
- Chain-of-Thought Prompting Elicits Reasoning in Large Language Models - <a href="https://arxiv.org/pdf/2201.11903">https://arxiv.org/pdf/2201.11903</a><br>
- Large Language Models can Strategically Deceive their Users when Put Under Pressure - <a href="https://arxiv.org/pdf/2311.07590">https://arxiv.org/pdf/2311.07590</a><br>
- Answering Questions by Meta-Reasoning over Multiple Chains of Thought - <a href="https://arxiv.org/pdf/2304.13007">https://arxiv.org/pdf/2304.13007</a><br>
- Toolformer: Language Models Can Teach Themselves to Use Tools - <a href="https://arxiv.org/pdf/2302.04761">https://arxiv.org/pdf/2302.04761</a><br>
- ReAct: Synergizing Reasoning and Acting in Language Models - <a href="https://arxiv.org/pdf/2210.03629">https://arxiv.org/pdf/2210.03629</a><br>
- Reflexion: Language Agents with Verbal Reinforcement Learning - <a href="https://arxiv.org/pdf/2303.11366">https://arxiv.org/pdf/2303.11366</a>
</details>
<hr>
<h2 id="diffusion-and-multimodal-models-implementation">Diffusion and Multimodal Models Implementation<a hidden class="anchor" aria-hidden="true" href="#diffusion-and-multimodal-models-implementation">#</a></h2>
<p><img alt="Diffusion Models" loading="lazy" src="/assets/images/diffusion-image.png"></p>
<p>A comprehensive implementation of a 2 MLP layer + U-Net diffusion models and multimodal architectures from scratch. This project features implementations of Denoising Diffusion Probabilistic Models (DDPM), Denoising Diffusion Implicit Models (DDIM), and CLIP (Contrastive Language-Image Pre-training). The codebase includes sophisticated U-Net architectures with attention mechanisms, classifier and CLIP guidance techniques for conditional generation, and various sampling methods.</p>
<p>Built with PyTorch and integrated with Weights &amp; Biases for experiment tracking.</p>
<p><a href="https://github.com/michaelyliu6/diffusion-models">View Project on GitHub</a></p>
<details>
<summary>Image Prompt</summary>
<i>An anime-style tech laboratory scene visualizing diffusion image generation. A central anime character with digital glasses operates a futuristic console labeled 'DIFFUSION MODEL' with multiple screens showing the same image at different denoising steps. The main display shows a 3D visualization of probability space with , where noise visibly transforms into multiple diverse anime images: HD of a cat, a cartoon of a friendly robot, and high definition landscape. Each generation step is marked with glowing nodes on an upward path, with t=1000 at the bottom (pure noise) and t=0 at the peak (clear images). The noise-to-image transition is clearly shown as particles coalescing into recognizable forms as they ascend the probability gradient. Floating holographic displays around the console show close-ups of the denoising process: one display shows sequential image frames evolving from static to clarity, another shows a visual representation of noise prediction at each step. A third display shows a heat map of where the model is focusing its attention during the current denoising step. The character manipulates particle streams flowing between time steps, with each stream containing tiny image fragments that become progressively more defined as they approach t=0. The lighting transitions from chaotic blue-purple for the noisy regions to structured golden light for the final image. The laboratory walls display animated equations and diagrams specifically showing the forward and reverse diffusion processes, with arrows indicating the direction of optimization. Above it all, a banner reads 'Denoising Diffusion Probabilistic Model' in stylized anime text. The scene includes multiple small denoising stages visible as floating platforms, each showing the diverse anime images getting clearer as the algorithm climbs toward the optimal distribution at the summit. Small holographic labels identify key concepts in the diffusion process: 'noise prediction,' 'variance scheduling,' and 'sampling path optimization.'</i> - Generated by Flux 1.1 Pro
<br><br>
</details>
<details>
<summary>References</summary>
- ML for Alignment Bootcamp (MLAB) - <a href="https://github.com/Kiv/mlab2">https://github.com/Kiv/mlab2</a><br>
- Denoising Diffusion Probabilistic Models - <a href="https://arxiv.org/pdf/2006.11239">https://arxiv.org/pdf/2006.11239</a><br>
- Denoising Diffusion Implicit Models - <a href="https://arxiv.org/pdf/2010.02502">https://arxiv.org/pdf/2010.02502</a><br>
- Learning Transferable Visual Models From Natural Language Supervision - <a href="https://arxiv.org/pdf/2103.00020">https://arxiv.org/pdf/2103.00020</a>
</details>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:55382/">ML&#39;s Blog</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
