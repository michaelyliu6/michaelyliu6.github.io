<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on ML&#39;s Blog</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on ML&#39;s Blog</description>
    <generator>Hugo -- 0.140.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 16 Feb 2025 07:07:07 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why &#34;real&#34; Reinforcement Learning will create the strongest technical moats</title>
      <link>http://localhost:1313/posts/rl-predictions/</link>
      <pubDate>Sun, 16 Feb 2025 07:07:07 +0100</pubDate>
      <guid>http://localhost:1313/posts/rl-predictions/</guid>
      <description>&lt;p&gt;The AI landscape has undergone rapid shifts in recent years. While 2023-2024 saw the commoditization of pre-training and supervised fine-tuning, 2025 will mark the emergence of &amp;ldquo;real&amp;rdquo; Reinforcement Learning (RL) as the primary technical moat in AI development. Unlike pre-training, which focuses on learning statistical correlations from massive datasets, RL allows models to actively explore solution spaces and discover novel strategies that generalize beyond static training data.&lt;/p&gt;
&lt;h2 id=&#34;the-limitations-of-rlhf-and-the-promise-of-real-rl&#34;&gt;The Limitations of RLHF and the Promise of &amp;ldquo;Real&amp;rdquo; RL&lt;/h2&gt;
&lt;p&gt;Unlike RLHF (Reinforcement Learning from Human Feedback), which optimizes for human approval rather than actual task performance, genuine RL with sparse rewards will enable models to solve complex end-to-end tasks autonomously. RLHF is fundamentally limited because it optimizes for a proxy objective (what looks good to humans) rather than directly solving problems correctly. Furthermore, models quickly learn to game reward models when trained with RLHF for extended periods. In contrast, true RL with sparse rewards—similar to what powered AlphaGo&amp;rsquo;s breakthrough—will create significant competitive advantages for several reasons.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Training Language Models with Reinforcement Learning from Human Feedback</title>
      <link>http://localhost:1313/posts/rlhf/</link>
      <pubDate>Sat, 15 Feb 2025 07:07:07 +0100</pubDate>
      <guid>http://localhost:1313/posts/rlhf/</guid>
      <description>&lt;p&gt;Reinforcement Learning from Human Feedback (RLHF) is a technique used to fine-tune large language models (LLMs) to better align with human preferences. It involves training a reward model based on human feedback and then using reinforcement learning to optimize the LLM&amp;rsquo;s policy to maximize the reward.&lt;/p&gt;
&lt;p&gt;This process generally involves three key steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Supervised Fine-tuning (SFT):&lt;/strong&gt; An initial language model is fine-tuned on a dataset of high-quality demonstrations, where the model learns to imitate the provided examples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Toy Diffusion Model</title>
      <link>http://localhost:1313/posts/toy_diffusion_model/</link>
      <pubDate>Fri, 20 Dec 2024 07:07:07 +0100</pubDate>
      <guid>http://localhost:1313/posts/toy_diffusion_model/</guid>
      <description>&lt;h2 id=&#34;what-even-is-diffusion&#34;&gt;What even is Diffusion?&lt;/h2&gt;
&lt;p&gt;Diffusion models approach generative modeling by mapping out probability distributions in high-dimensional spaces. Consider our dataset as a tiny sample from an enormous space of possible images. Our goal is to estimate which regions of this vast space have high probability according to our target distribution.&lt;/p&gt;
&lt;p&gt;The core insight of diffusion is that if we add Gaussian noise to an image from our distribution, the resulting noisy image typically becomes less likely to belong to that distribution. This is an empirical observation about human perception - a shoe with a small amount of noise still looks like a shoe, but becomes less recognizable as more noise is added.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
