<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on ML&#39;s Blog</title>
    <link>http://localhost:55382/posts/</link>
    <description>Recent content in Posts on ML&#39;s Blog</description>
    <generator>Hugo -- 0.140.0</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Mar 2025 10:00:00 -0500</lastBuildDate>
    <atom:link href="http://localhost:55382/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Claude Plays Pokemon</title>
      <link>http://localhost:55382/posts/claude-plays-pokemon/</link>
      <pubDate>Mon, 03 Mar 2025 10:00:00 -0500</pubDate>
      <guid>http://localhost:55382/posts/claude-plays-pokemon/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: This blog post was written based on the initial Claude Plays Pokemon livestream. The implementation appears to have changed since this analysis was written with new tools and potential new prompts. For the most up-to-date information, please refer to the latest livestream recordings.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;With the release of Claude 3.7 Sonnet, Anthropic also released a first of its kind benchmark: Claude playing Pokemon Red. Despite never being explicitly trained to play the game, Claude was still able to make progress through the game and even getting the Surge&amp;rsquo;s badge (the 3rd badge in the game). Having grown up deeply invested in playing the Pokemon series games, I have very fond and intense memories from playing the game, so I wanted to take a deep dive into Claude&amp;rsquo;s experience playing the game. In this post, I will be deep diving into the scaffolding of the system and the tools that help Claude play the game and analyze how well it does.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why &#34;real&#34; Reinforcement Learning will create the strongest technical moats</title>
      <link>http://localhost:55382/posts/rl-predictions/</link>
      <pubDate>Sun, 16 Feb 2025 07:07:07 +0100</pubDate>
      <guid>http://localhost:55382/posts/rl-predictions/</guid>
      <description>&lt;p&gt;The AI landscape has undergone rapid shifts in recent years. While 2023-2024 saw the commoditization of pre-training and supervised fine-tuning, 2025 will mark the emergence of &amp;ldquo;real&amp;rdquo; Reinforcement Learning (RL) as the primary technical moat in AI development. Unlike pre-training, which focuses on learning statistical correlations from massive datasets, RL allows models to actively explore solution spaces and discover novel strategies that generalize beyond static training data.&lt;/p&gt;
&lt;h2 id=&#34;the-limitations-of-rlhf-and-the-promise-of-real-rl&#34;&gt;The Limitations of RLHF and the Promise of &amp;ldquo;Real&amp;rdquo; RL&lt;/h2&gt;
&lt;p&gt;Unlike RLHF (Reinforcement Learning from Human Feedback), which optimizes for human approval rather than actual task performance, genuine RL with sparse rewards will enable models to solve complex end-to-end tasks autonomously. RLHF is fundamentally limited because it optimizes for a proxy objective (what looks good to humans) rather than directly solving problems correctly. Furthermore, models quickly learn to game reward models when trained with RLHF for extended periods. In contrast, true RL with sparse rewards—similar to what powered AlphaGo&amp;rsquo;s breakthrough—will create significant competitive advantages for several reasons.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Training Language Models with Reinforcement Learning from Human Feedback</title>
      <link>http://localhost:55382/posts/rlhf/</link>
      <pubDate>Sat, 15 Feb 2025 07:07:07 +0100</pubDate>
      <guid>http://localhost:55382/posts/rlhf/</guid>
      <description>&lt;p&gt;Reinforcement Learning from Human Feedback (RLHF) is a technique used to fine-tune large language models (LLMs) to better align with human preferences. It involves training a reward model based on human feedback and then using reinforcement learning to optimize the LLM&amp;rsquo;s policy to maximize the reward.&lt;/p&gt;
&lt;p&gt;This process generally involves three key steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Supervised Fine-tuning (SFT):&lt;/strong&gt; An initial language model is fine-tuned on a dataset of high-quality demonstrations, where the model learns to imitate the provided examples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Toy Diffusion Model</title>
      <link>http://localhost:55382/posts/toy_diffusion_model/</link>
      <pubDate>Fri, 10 Jan 2025 07:07:07 +0100</pubDate>
      <guid>http://localhost:55382/posts/toy_diffusion_model/</guid>
      <description>&lt;h2 id=&#34;what-even-is-diffusion&#34;&gt;What even is Diffusion?&lt;/h2&gt;
&lt;p&gt;Diffusion models approach generative modeling by mapping out probability distributions in high-dimensional spaces. Consider our dataset as a tiny sample from an enormous space of possible images. Our goal is to estimate which regions of this vast space have high probability according to our target distribution.&lt;/p&gt;
&lt;p&gt;The core insight of diffusion is that if we add Gaussian noise to an image from our distribution, the resulting noisy image typically becomes less likely to belong to that distribution. This is an empirical observation about human perception - a shoe with a small amount of noise still looks like a shoe, but becomes less recognizable as more noise is added.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ML Papers</title>
      <link>http://localhost:55382/posts/ml-papers/</link>
      <pubDate>Sat, 14 Dec 2024 07:07:07 +0100</pubDate>
      <guid>http://localhost:55382/posts/ml-papers/</guid>
      <description>&lt;h1 id=&#34;ai-alignment&#34;&gt;AI Alignment&lt;/h1&gt;
&lt;h2 id=&#34;constitutional-classifiers-defending-against-universal-jailbreaks-across-thousands-of-hours-of-red-teaming&#34;&gt;Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming&lt;/h2&gt;
&lt;p&gt;Anthropic (Jan 2025) &lt;a href=&#34;https://arxiv.org/pdf/2501.18837&#34;&gt;https://arxiv.org/pdf/2501.18837&lt;/a&gt;
&lt;img src=&#34;http://localhost:55382/assets/images/ml-papers/constitutional-classifiers.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;alignment-faking-in-large-language-models&#34;&gt;Alignment Faking in Large Language Models&lt;/h2&gt;
&lt;p&gt;Anthropic (Dec 2024) &lt;a href=&#34;https://arxiv.org/pdf/2412.14093&#34;&gt;https://arxiv.org/pdf/2412.14093&lt;/a&gt;
&lt;img src=&#34;http://localhost:55382/assets/images/ml-papers/alignment-faking.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;constitutional-ai-harmlessness-from-ai-feedback&#34;&gt;Constitutional AI: Harmlessness from AI Feedback&lt;/h2&gt;
&lt;p&gt;Anthropic (Dec 2022) &lt;a href=&#34;https://arxiv.org/pdf/2212.08073&#34;&gt;https://arxiv.org/pdf/2212.08073&lt;/a&gt;
&lt;img src=&#34;http://localhost:55382/assets/images/ml-papers/constitutional-ai.png&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;pre-training&#34;&gt;Pre-training&lt;/h1&gt;
&lt;h2 id=&#34;deepseek-v2-a-strong-economical-and-efficient-mixture-of-experts-language-model&#34;&gt;DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model&lt;/h2&gt;
&lt;p&gt;DeepSeek (June 2024) &lt;a href=&#34;https://arxiv.org/pdf/2405.04434&#34;&gt;https://arxiv.org/pdf/2405.04434&lt;/a&gt;
&lt;img src=&#34;http://localhost:55382/assets/images/ml-papers/deepseek-v2.png&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;reinforcement-learning&#34;&gt;Reinforcement Learning&lt;/h1&gt;
&lt;h2 id=&#34;deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning&#34;&gt;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&lt;/h2&gt;
&lt;p&gt;DeepSeek (Jan 2025) &lt;a href=&#34;https://arxiv.org/pdf/2501.12948&#34;&gt;https://arxiv.org/pdf/2501.12948&lt;/a&gt;
&lt;img src=&#34;http://localhost:55382/assets/images/ml-papers/deepseek-r1.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;deepseekmath-pushing-the-limits-of-mathematical-reasoning-in-open-language-models&#34;&gt;DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models&lt;/h2&gt;
&lt;p&gt;DeepSeek (Apr 2024) &lt;a href=&#34;https://arxiv.org/pdf/2401.06066&#34;&gt;https://arxiv.org/pdf/2401.06066&lt;/a&gt;
&lt;img src=&#34;http://localhost:55382/assets/images/ml-papers/grpo.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding Modern LLM Architectures with DeepSeek-V3</title>
      <link>http://localhost:55382/posts/gpt2-to-deepseekv3/</link>
      <pubDate>Sat, 14 Dec 2024 07:07:07 +0100</pubDate>
      <guid>http://localhost:55382/posts/gpt2-to-deepseekv3/</guid>
      <description>&lt;h1 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-architecture&#34;&gt;Model Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;DeepSeek has quickly become a household name after their release of DeepSeek-R1 model which caused dramatic sell off of NVIDIA stock, causing its value to drop by roughly 17% in one trading session, erasing $593 billion in market value, a record one-day loss for any company on Wall Street (&lt;a href=&#34;https://www.reuters.com/technology/chinas-deepseek-sets-off-ai-market-rout-2025-01-27/&#34;&gt;Reuters&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The intended audience of this post is someone who has a solid understanding of Deep Learning and GPT2.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI Alignment</title>
      <link>http://localhost:55382/posts/alignment/</link>
      <pubDate>Thu, 15 Feb 2024 07:07:07 +0100</pubDate>
      <guid>http://localhost:55382/posts/alignment/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;h2 id=&#34;capabilities-vs-alignment&#34;&gt;Capabilities vs Alignment&lt;/h2&gt;
&lt;p&gt;The field of AI safety, there is an important distinction between &lt;strong&gt;capabilities evaluations&lt;/strong&gt; and &lt;strong&gt;alignment evaluations&lt;/strong&gt;. &lt;strong&gt;Capabilities evaluations&lt;/strong&gt; measures the the capacity  (i.e. what the model &amp;ldquo;can&amp;rdquo; do), such as its proficiency at logical reasoning, coding, or writing. &lt;strong&gt;Alignment evaluations&lt;/strong&gt; measures the tendency for models to exhibit certain behaviors (i.e. what the model &amp;ldquo;wants&amp;rdquo; to do), such as being helpful, honest, and harmless, &lt;a href=&#34;https://en.wikipedia.org/wiki/Sycophancy&#34;&gt;sycophantic&lt;/a&gt;, or deceptive.&lt;/p&gt;
&lt;p&gt;We can conceptualize different combinations of capabilities and alignment:&lt;/p&gt;</description>
    </item>
    <item>
      <title>DeepSeek-VL2</title>
      <link>http://localhost:55382/posts/deepseek-vl2/</link>
      <pubDate>Sun, 14 Jan 2024 07:07:07 +0100</pubDate>
      <guid>http://localhost:55382/posts/deepseek-vl2/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;ref/DeepSeek-VLM2.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;b&lt;/code&gt;: Batch size&lt;/li&gt;
&lt;li&gt;&lt;code&gt;T&lt;/code&gt;: Original text sequence length&lt;/li&gt;
&lt;li&gt;&lt;code&gt;T&#39;&lt;/code&gt;: Text sequence length + image tokens&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max_n_images&lt;/code&gt;: Maximum number of images&lt;/li&gt;
&lt;li&gt;&lt;code&gt;H, W&lt;/code&gt;: Original image height/width&lt;/li&gt;
&lt;li&gt;&lt;code&gt;h, w&lt;/code&gt;: Cropped image height/width&lt;/li&gt;
&lt;li&gt;&lt;code&gt;num_patches&lt;/code&gt;: Number of patches (Vision Encoder)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;embed_dim&lt;/code&gt;: Vision Encoder embedding dimension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;D&lt;/code&gt;: Language model embedding dimension&lt;/li&gt;
&lt;li&gt;&lt;code&gt;num_tiles&lt;/code&gt;: Total number of image tiles&lt;/li&gt;
&lt;li&gt;&lt;code&gt;prefix_tokens&lt;/code&gt;: Number of class tokens&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;vlm-architecture&#34;&gt;VLM Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;ref/DeepSeek-VLM-arch2.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s break down the full autoregressive workflow of DeepSeek-VL2, step-by-step, in words. This will cover both the initial processing of the prompt (which can be optimized with incremental prefilling) and the subsequent token-by-token generation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>My First Post</title>
      <link>http://localhost:55382/posts/my-first-post/</link>
      <pubDate>Sun, 14 Jan 2024 07:07:07 +0100</pubDate>
      <guid>http://localhost:55382/posts/my-first-post/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hi there! My name is Michael Liu.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding Agent Capabilities through the GAIA Benchmark</title>
      <link>http://localhost:55382/posts/agents/</link>
      <pubDate>Sun, 14 Jan 2024 07:07:07 +0100</pubDate>
      <guid>http://localhost:55382/posts/agents/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;h2 id=&#34;gaia-benchmark-quantifying-real-world-agent-progress&#34;&gt;GAIA Benchmark: Quantifying Real-World Agent Progress&lt;/h2&gt;
&lt;p&gt;Evaluating AI agents necessitates benchmarks that transcend academic exercises and accurately reflect practical, real-world capabilities. While datasets like &lt;strong&gt;MMLU&lt;/strong&gt; assess knowledge domains, they do not fully capture the essential skills for effective agents in complex, everyday scenarios. The &lt;strong&gt;GAIA&lt;/strong&gt; (\textbf{G}eneral \textbf{AI} \textbf{A}ssistants) benchmark \citep{mialon2023gaia} provides a more pertinent evaluation: it challenges agents with tasks demanding &lt;strong&gt;reasoning&lt;/strong&gt;, &lt;strong&gt;tool utilization&lt;/strong&gt;, and &lt;strong&gt;multi-modal comprehension&lt;/strong&gt; within open-ended, real-world contexts.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
