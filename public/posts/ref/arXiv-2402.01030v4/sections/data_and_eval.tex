\looseness=-1
\section{Empowering Open-source LLM Agent to be Better at \approach}
\label{sec:data_and_evaluation}
\label{sec:llm_agent_framework_capability}

The promising results achieved by \approach motivate us to build an open-source LLM agent that can both interact with environments through \approach and communicate with humans using language.
% 
To improve open-source LLMs' \approach capability, in \sref{sec:agent_env_data}, we introduce \dataname, an instruction finetuning dataset that contains agent-environment interaction trajectories.
% 
We discuss data selection procedures in \sref{sec:agent_env_data_self_improve} to promote improvement from interaction behavior.
% 
Additionally, we show that \approach can be used together with existing agent-user conversation data (\sref{sec:agent_user_data}) to balance the dialog capability of the resulting LLM.
% 
Our model \modelname, finetuned from LLaMA-2 \citep{touvron2023llama} and Mistral-7B \citep{jiang2023mistral} on a mixture of \dataname and general conversations, improves \approach performances without hurting LLM's general performance on a diverse suite of tasks (\sref{sec:llm_agent_evaluation}).

\subsection{\dataname: Agent-Environment Interactions}
\label{sec:agent_env_data}

We consider four main use cases in agent-environment interaction and repurpose five existing datasets across different domains to generate trajectories:

\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*]
    \item \textbf{Information Seeking:} We use a training subset of HotpotQA \citep{yang2018hotpotqa} to generate information-seeking trajectories, where LLMs use the \texttt{wikipedia\_search} API (provided as a Python function) to search for information to answer questions.
    
    \item \textbf{Software Package (Tool) Usage:} We use the training set of code generation problems in APPS \citep{hendrycks2021measuring} and math problems in MATH \citep{hendrycks2021math}. The code generation tasks already involve importing packages and/or creating new tools by defining a new Python function. For MATH, we provide an in-context demonstration of importing Python packages (e.g., \texttt{sympy} for symbolic math) for problem-solving.
    
    \item \textbf{External Memory:} We repurpose the training subset of WikiTableQuestion \citep{pasupat2015compositional} and tweak it into two variants of tabular reasoning tasks that require accessing external memory: (1) SQL-based, requiring the LLM to interact with an SQL database through sqlite3 package to answer the question via SQL execution; (2) Pandas-based, requiring the model to interact with pandas tables to perform data operations (e.g., select, filter). Examples of instructions can be found in \sref{sec:interaction_data_gen_prompt_tabular}.
    % \footnote{\url{https://docs.python.org/3/library/sqlite3.html}}
    % \footnote{\url{https://pandas.pydata.org/}}

    \item \textbf{Robot Planning:} We use ALFWorld \citep{shridhar2020alfworld}, a text-only embodied environment simulator, to generate trajectories that use robot-control APIs (repurposed as Python function) to complete household tasks. Following MINT \citep{wang2023mint}, we provide an in-context demonstration to encourage the use of for-loop and if-statement code blocks to automate repetitive operations (e.g., searching for items by visiting different locations).
    
\end{itemize}

\noindent \textbf{Data Down-sampling.}
% 
We down-sample each dataset by keeping only the most challenging instances, aiming to make trajectory generation more efficient and cost-effective. Furthermore, it also helps remove simple instances that existing LLMs can already solve.
The statistics of the filtered dataset can be found in \tref{tab:data_training_instances}. Please refer to \sref{sec:how_to_downsample_dataset} for details about the down-sample process.

\noindent \textbf{Repurpose Data for Multi-turn Interaction.}
Some datasets (APPS, MATH, WikiTableQuestions) are initially single-turn problems that expect \textit{one} solution per instruction, whereas, in a realistic agent use case, we often require multi-turn interaction to complete each task (\fref{fig:illustrative_example} top).
% 
Following MINT \citep{wang2023mint}, we repurpose single-turn problems into multi-turn ones by allowing LLM to interact with the environment for multiple turns before it decides to submit one solution for evaluation.
% 
Specifically for code generation problems, we provide an in-context example to guide LLMs to test their solution on provided test cases before they submit the solution.
% 
Metrics from the original data will evaluate the submitted solution to determine its correctness. We include examples in \sref{sec:interaction_data_gen_prompt}.



\noindent \textbf{Trajectory Generation.}
% 
We use MINT's evaluation framework \citep{wang2023mint} to generate interaction trajectories for the aforementioned datasets and determine the correctness of each trajectory.
% \footnote{\url{https://beta.openai.com/}}
% \footnote{\url{https://www.anthropic.com/}}
We run gpt-3.5-turbo-0613 from OpenAI, claude-1-instant and claude-2 from Anthropic on down-sampled data, except code generation, which we use a longer-context version of GPT-3.5 (gpt-3.5-turbo-0613-16k) due to the long-context requirement of the self-debugging process.
% 
On a subset of problems that none of these models can solve, we use gpt-4-0613 to generate trajectories.


\noindent \textbf{Enhancing Agent's Capabilities of Improving from Interaction.}
\label{sec:agent_env_data_self_improve}
% 
We select a high-quality subset of all the generated trajectories from \dataname to promote the agent's ability to improve the next action based on prior observations (e.g., self-debugging from code execution error message, a planning capability in \fref{fig:llm_agent_framework}).
% 
To achieve this, we selectively preserve those trajectories wherein the model initially encounters errors but rectifies these inaccuracies in later interactions.
% 
For these instances, the LLM typically engages in self-reflection following the initial error, thereby proactively enhancing its future actions. 
% 
Other filtering details are discussed in \sref{sec:data_filter_heuristic}.
% 
On all trajectories generated, we keep 411 trajectories from gpt-4-0613 and 6728 trajectories from gpt-3.5 and claude. 
The statistics of the resulting dataset \dataname are shown in \tref{tab:data_training_mixture_stats}.

\begin{table*}[!ht]
% \vspace{-10pt}
\centering
\caption{Statistics of our training mixture and comparison with prior work. Please refer to \sref{sec:agent_env_data} for details about \dataname and general conversation data. Token statistics are computed using Llama-2 tokenizer.
}
\vspace{-9pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{lrrrrr}
\toprule
\textbf{Data Mixture} & \textbf{Data Type} & \textbf{Data Name} & \textbf{\# of Data Instances} &  \textbf{\# of Total Tokens} & \textbf{Avg. Tokens Per Instance} \\
\midrule
\multirow{2}{*}{\textbf{Prior Work}}& - & FireAct \citep{chen2023fireact} &   $2,063$ &    $542,176$ &   $262.81$ \\
 &  - & AgentInstruct \citep{zeng2023agenttuning} &    $1,866$ &   $2,517,785$ &  $1349.30$ \\
\midrule

\multirow{6}{*}{\textbf{\dataname} (Ours)}
& Information Seeking & HotpotQA \citep{yang2018hotpotqa} &      $1,664$ &   $2,472,227$ &  $1485.71$ \\
& Software Packages (Tool) & MATH (Math, \citep{hendrycks2021math}) &      $1,732$ &   $1,719,467$ &   $992.76$ \\
& Software Packages (Tool) & APPS (Code, \citep{hendrycks2021measuring}) &        $647$ &   $1,235,472$ &  $1909.54$ \\
& External Memory & WikiTableQuestion \citep{pasupat2015compositional} &      $1,065$ &   $1,316,246$ &  $1235.91$ \\
& Robot Planning & ALFWorld \citep{shridhar2020alfworld} &      $2,031$ &   $3,838,269$ &  $1889.84$ \\
\cmidrule{2-6}
& \multicolumn{2}{r}{\textbf{Total}} &      $\mathbf{7,139}$ &  $\mathbf{10,581,681}$ &  $\mathbf{1482.24}$ \\

\midrule

\multirow{5}{*}{\textbf{General Conversation}}
& Single-Turn Reasoning & OpenOrca (Sub-sampled, \citep{OpenOrca}) &     $50,000$ &  $14,034,152$ &   $280.68$ \\
& Multi-Turn Conversations & ShareGPT (Sub-sampled, \citep{sharegpt_dataset}) &     $10,000$ &  $17,933,861$ &  $1793.39$ \\
& Multi-Turn Conversations & ShareGPT (GPT-4, \citep{sharegpt4_dataset}) &      $4,583$ &  $18,195,878$ &  $3970.30$ \\
& Multi-turn Reasoning & CapyBara \citep{capybara_dataset} &  $4,647$ &   $4,982,435$ &  $1072.18$ \\
\cmidrule{2-6}
& \multicolumn{2}{r}{\textbf{Total}} &     $\mathbf{69,230}$ &  $\mathbf{55,146,326}$ &   $\mathbf{796.57}$ \\

\bottomrule
\end{tabular}
}
\label{tab:data_training_mixture_stats}
% \vspace{-3pt}
\end{table*}


\noindent \textbf{Comparing \dataname with Prior Work.}
Compared with prior work AgentInstruct \citep{zeng2023agenttuning} and FireAct \citep{chen2023fireact} that mainly focus using text as action, \dataname results in models that are more practical in real-world implementation, as such models using \approach can directly interact with Python interpreters and open-source toolkits (\fref{fig:qualitative_example}), reducing the development effort for action parsing and tool creations.
% 
\dataname is systematically constructed following the general agent framework (\fref{fig:llm_agent_framework}).
% 
It covers diverse domains (e.g., compared to FireAct that only considers QA-task and search API), contains quality data (e.g., promotes agent's capability of self-debug) and of larger size (3.8x / 3.5x more data trajectories and 5x / 19x more tokens compared to AgentInstruct / FireAct respectively in \tref{tab:data_training_mixture_stats}). 
% 
As we empirically show in \tref{tab:model_benchmark_results}, the resulting model (same backbone) of \dataname achieves 24\% and 119\% relative improvement compared to AgentInstruct and FireAct.


\noindent \textbf{\dataname Can Be Used With Existing Agent-User Conversation Data.}
\label{sec:agent_user_data}
We use a sub-sampled set of OpenOrca \citep{OpenOrca} that focuses on single-turn chain-of-thought (CoT) reasoning, ShareGPT \citep{sharegpt_dataset, sharegpt4_dataset} from two sources that contain multi-turn conversations between human and LLM, and CapyBara \citep{capybara_dataset} that focuses on reasoning in multi-turn conversations.
%
Statistics and down-sampling details can be found in \tref{tab:data_training_mixture_stats} and \sref{sec:general_data_downsample}.
% 
% Please refer to \tref{tab:data_training_mixture_stats} for statistics of general conversations.


{
\begin{table*}[!t]
\centering
% \vspace{-5pt}
\caption{
Evaluation results for \modelname. The best results among all open-source LLMs are \textbf{bolded}, and the second-best results are \underline{underlined}.
% 
ID and OD stand for in-domain and out-of-domain evaluation correspondingly.
% 
Overall averaged performance normalizes the MT-Bench score to be consistent with other tasks and excludes in-domain tasks for fair comparison.
}
\vspace{-10pt}
\begin{threeparttable}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{@{} lc rrr m{0.01em} rr m{0.01em} rrrr m{0.01em} r@{}}
\toprule
& & \multicolumn{6}{c}{\textbf{Agent Tasks}}                                                         && \multicolumn{4}{c}{\textbf{Generic Tasks}} && \textbf{Overall} \\
 \cmidrule{3-8}
& & \multicolumn{3}{c}{\textbf{Code as Action}} && \multicolumn{2}{c}{\textbf{Text as Action (OD)}} && \multicolumn{4}{c}{\textbf{(OD)}} && \textbf{Average}\\
  \cmidrule{3-5}
  \cmidrule{7-8}
  \cmidrule{10-14}
 Model & Size & MINT (ID) & MINT (OD) & \evalname (OD) && Miniwob++ & SciWorld && MMLU & HumanEval & GSM8K & MTBench & \\
\midrule


\multicolumn{15}{c}{\textit{Open-source LLMs (LLaMA-2-based)}} \\

Llama2 Base & 7B & -$^*$ & -$^*$ & -$^*$ &  & -$^*$ & -$^*$ &  & $45.3$ & $12.8$ & $14.6$ & -$^*$ &  & -$^*$ \\
Llama2 Chat & 7B & $3.2$ & $11.0$ & \underline{$0.0$} &  & $0.0$ & $5.9$ &  & $48.0$ & $13.9$ & $27.7$ & $6.3$ &  & $21.1$ \\
FireAct \citep{chen2023fireact} & 7B & $0.0$ & $0.3$ & \underline{$0.0$} &  & $0.0$ & $6.8$ &  & $44.1$ & $3.5$ & $12.4$ & $4.5$ &  & $14.0$ \\
AgentLM \citep{zeng2023agenttuning} & 7B & $8.7$ & $6.1$ & \underline{$0.0$} &  & \underline{$28.9$} & $13.7$ &  & $48.7$ & $15.4$ & $24.6$ & $6.1$ &  & $24.8$ \\
\modelname (LLaMA-2) & 7B & \underline{$51.3$} & \underline{$20.4$} & \underline{$0.0$} &  & $25.5$ & $\mathbf{17.6}$ &  & $50.6$ & $18.1$ & $38.3$ & \underline{$7.5$} &  & \underline{$30.7$} \\


\midrule
\multicolumn{15}{c}{\textit{Open-source LLMs (Mistral-based)}} \\

Mistral Base & 7B & -$^*$ & -$^*$ & -$^*$ &  & -$^*$ & -$^*$ &  & $\mathbf{60.1}$ & \underline{$30.5$} & \underline{$52.1$} & -$^*$ &  & -$^*$ \\
Mistral Instruct & 7B & $18.8$ & $9.7$ & \underline{$0.0$} &  & $0.5$ & $4.0$ &  & $53.8$ & $29.3$ & $43.3$ & $6.4$ &  & $25.6$ \\
\modelname (Mistral) & 7B & $\mathbf{57.4}$ & $\mathbf{32.4}$ & $\mathbf{12.2}$ &  & $\mathbf{46.2}$ & \underline{$15.9$} &  & \underline{$59.1$} & $\mathbf{34.7}$ & $\mathbf{58.0}$ & $\mathbf{8.2}$ &  & $\mathbf{42.5}$ \\

\midrule
\multicolumn{15}{c}{\textit{Closed-source LLMs}} \\

gpt-3.5-turbo-0613 & - & $33.9$ & $38.2$ & $51.2$ &  & $66.7$ & $21.2$ &  & $70.0$ & $48.1$ & $57.1$ & $7.9$ &  & $54.0$ \\
gpt-4-0613 & - & $68.6$ & $70.2$ & $67.1$ &  & $69.4$ & $36.4$ &  & $86.4$ & $67.0$ & $87.1$ & $9.0$ &  & $71.7$ \\

\bottomrule
\end{tabular}
\end{adjustbox}
{
\small
\begin{tablenotes}
    \item[*] Some results are only available with instruction-tuned models.
\end{tablenotes}
}
\end{threeparttable}
\label{tab:model_benchmark_results}
\vspace{-15pt}
\end{table*}
}

\subsection{\modelname}
\label{sec:llm_agent_evaluation}

We fine-tune Llama-2 7B \citep{touvron2023llama} and Mistral 7B \citep{jiang2023mistral} on a mixture of \dataname and general conversations (\tref{tab:data_training_mixture_stats}) to obtain \modelname.

\noindent \textbf{Training Setup.} We perform full-parameter supervised fine-tuning with a sequence length of 4,096 tokens for Llama-2 and 16,384 for Mistral. Please refer to \sref{sec:model_training_details} for more details.

\noindent \textbf{Evaluation Setup.} 
% 
We use MINT \citep{wang2023mint} to evaluate LLMs with \approach on a diverse range of agent tasks.
% 
\modelname has some training domains overlapping with MINT's evaluation (i.e., MINT includes ALFWorld and MATH), hence we report separate numbers for MINT's in- and out-of-domain performance.
%
Unless otherwise specified, we measure MINT tasks' success rates with interaction turn $k=5$.
% 
We also evaluate out-of-domain agent tasks using text actions from MiniWob++ (computer tasks, \cite{kim2023language}) and ScienceWorld (text-based simulator for elementary science curriculum, \cite{Wang2022ScienceWorldIY}) to test whether \modelname can generalize to different action formats.
% 
Finally, we include a suite of general LLM evaluation tasks to assess general capability: MMLU \citep{hendrycks2020measuring} for knowledge-based QA, HumanEval \citep{chen2021evaluating} for single-turn code-generation, GSM8K \citep{cobbe2021training} for single-turn tool-free math reasoning, and MTBench \citep{zheng2023judging} for instruction-following.

\noindent \textbf{\modelname Excels in \approach Task.}
% 
As shown in \tref{tab:model_benchmark_results}, \modelname (both variants) perform better than all evaluated open-source LLMs on both the in- and out-of-domain subsets of MINT.
% 
On \evalname, we find \modelname (Mistral) outperforms open-source LLMs of similar size (7B and 13B) and even reaches similar performance to those 70B models (\tref{tab:zeroshot_act_results}).
Surprisingly, no improvement is observed for the Llama-2 variant. We discuss potential reasons in \sref{sec:llama_anomaly}. 


\noindent \textbf{\modelname Generalizes to Text Action.}
% 
When evaluated on out-of-domain text actions, \modelname (LLaMA2, 7B), which has never been optimized for text action, achieves comparable performance to AgentLM-7B \citep{zeng2023agenttuning} which has explicit tuning for text actions.

\noindent \textbf{\modelname Maintains or Improves the Performance on General LLM Tasks.}
%
In \tref{tab:model_benchmark_results}, we find that \modelname (both variants) performs better on generic LLM tasks we tested, except for a slight degradation on MMLU for \modelname (Mistral, 7B).


\noindent \textbf{Ablation Study.}
\tref{tab:ablation_study_results} presents ablation experiments to determine the importance of \dataname and general conversations.
Both \dataname and general conversations contribute to agent tasks, while general conversations are essential to maintain performance on general tasks.
% 
% Similar to the findings in \citet{zeng2023agenttuning}, general conversation data contributes to the performance of out-of-domain agent tasks with text format, especially on Miniwob++.
