\looseness=-1
Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges.
LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools).
This work proposes to use executable Python \textbf{code} to consolidate LLM agents' \textbf{act}ions into a unified action space (\approach).
Integrated with a Python interpreter, \approach can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions.
Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that \approach outperforms widely used alternatives (up to 20\% higher success rate).
The encouraging performance of \approach motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language.
To this end, we collect an instruction-tuning dataset \dataname that consists of 7k multi-turn interactions using \approach.
We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability.
\modelname, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug\footnote{The code, data, model, and demo are available at \url{https://github.com/xingyaoww/code-act}.}.
\vspace{-0.2cm}