<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=61478&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>DeepSeek-VL2 | ML&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="

b: Batch size
T: Original text sequence length
T&#39;: Text sequence length &#43; image tokens
max_n_images: Maximum number of images
H, W: Original image height/width
h, w: Cropped image height/width
num_patches: Number of patches (Vision Encoder)
embed_dim: Vision Encoder embedding dimension
D: Language model embedding dimension
num_tiles: Total number of image tiles
prefix_tokens: Number of class tokens

VLM Architecture

Let&rsquo;s break down the full autoregressive workflow of DeepSeek-VL2, step-by-step, in words. This will cover both the initial processing of the prompt (which can be optimized with incremental prefilling) and the subsequent token-by-token generation.">
<meta name="author" content="Michael Liu">
<link rel="canonical" href="http://localhost:61478/posts/deepseek-vl2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.ae03dc1363b0d274c8a5b8f4ef4b43ec376146d505cc14962ea16577e875c413.css" integrity="sha256-rgPcE2Ow0nTIpbj070tD7DdhRtUFzBSWLqFld&#43;h1xBM=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:61478/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:61478/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:61478/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:61478/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:61478/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:61478/posts/deepseek-vl2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$']],
    displayMath: [['\\[', '\\]'], ['$$', '$$']],
    processEscapes: true,
    processEnvironments: true
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  },
  startup: {
    pageReady: () => {
      return MathJax.startup.defaultPageReady().then(() => {
        console.log('MathJax initial typesetting complete');
      });
    }
  },
  chtml: {
    fontURL: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2'
  }
};
</script>
<script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

    <link rel="stylesheet" href="/css/custom.css">
    <script src="/js/zoom.js"></script> 
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:61478/" accesskey="h" title="ML&#39;s Blog (Alt + H)">ML&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:61478/posts/" title="Posts">
                    <span>Posts</span>
                    
                </a>
            </li>
            <li>
                <a href="http://localhost:61478/projects/" title="Projects">
                    <span>Projects</span>
                    
                </a>
            </li>
            <li>
                <a href="http://localhost:61478/learning-resources/" title="Learning Resources">
                    <span>Learning Resources</span>
                    
                </a>
            </li>
            <li>
                <a href="http://localhost:61478/about/" title="About">
                    <span>About</span>
                    
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:61478/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:61478/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      DeepSeek-VL2
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2024-01-14 07:07:07 +0100 +0100'>January 14, 2024</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Michael Liu

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="ref/DeepSeek-VLM2.png"></p>
<ul>
<li><code>b</code>: Batch size</li>
<li><code>T</code>: Original text sequence length</li>
<li><code>T'</code>: Text sequence length + image tokens</li>
<li><code>max_n_images</code>: Maximum number of images</li>
<li><code>H, W</code>: Original image height/width</li>
<li><code>h, w</code>: Cropped image height/width</li>
<li><code>num_patches</code>: Number of patches (Vision Encoder)</li>
<li><code>embed_dim</code>: Vision Encoder embedding dimension</li>
<li><code>D</code>: Language model embedding dimension</li>
<li><code>num_tiles</code>: Total number of image tiles</li>
<li><code>prefix_tokens</code>: Number of class tokens</li>
</ul>
<h3 id="vlm-architecture">VLM Architecture<a hidden class="anchor" aria-hidden="true" href="#vlm-architecture">#</a></h3>
<p><img loading="lazy" src="ref/DeepSeek-VLM-arch2.png"></p>
<p>Let&rsquo;s break down the full autoregressive workflow of DeepSeek-VL2, step-by-step, in words. This will cover both the initial processing of the prompt (which can be optimized with incremental prefilling) and the subsequent token-by-token generation.</p>
<p><strong>I. Prompt Processing (Prefilling)</strong></p>
<p>This phase prepares the model to start generating text. It takes the user&rsquo;s input (text and optionally images) and converts it into a form the model can understand.</p>
<ol>
<li>
<p><strong>User Input:</strong> The user provides:</p>
<ul>
<li>Text input (a question, a description, etc.).</li>
<li>Optionally, one or more images.</li>
</ul>
</li>
<li>
<p><strong>Preprocessing (DeepseekVLV2Processor):</strong></p>
<ul>
<li><strong>Text Tokenization:</strong> The input text is split into tokens (words or sub-words) using the tokenizer. Each token is converted into a numerical ID.</li>
<li><strong>Image Processing:</strong> If images are present:
<ul>
<li>The processor identifies occurrences of the special <code>&lt;image&gt;</code> token in the text.</li>
<li>Each image is potentially resized and cropped into multiple views:
<ul>
<li>A &ldquo;global&rdquo; view (the entire image, padded to a standard size).</li>
<li>Several &ldquo;local&rdquo; views (smaller, overlapping crops).</li>
</ul>
</li>
<li>Each view is transformed into a tensor (numerical representation of pixel values) and normalized.</li>
<li>A sequence of special image tokens (<code>&lt;image&gt;</code>) is created. The number of these tokens corresponds to the global view and the local views.</li>
<li>A <code>images_seq_mask</code> is generated to represent the location of the image tokens.</li>
<li><code>image_spatial_crop</code> records how to crop images.</li>
</ul>
</li>
<li><strong>Combined Token Sequence:</strong> The tokenized text and the image tokens are combined into a single sequence of IDs. The <code>&lt;image&gt;</code> tokens in the text are placeholders that will be replaced by the image embeddings later.</li>
</ul>
</li>
<li>
<p><strong>Embedding Preparation (DeepseekVLV2ForCausalLM.prepare_inputs_embeds):</strong> This step creates the actual input embeddings for the language model.</p>
<ul>
<li><strong>Text Embeddings:</strong> The token IDs for the text are converted into embedding vectors using the language model&rsquo;s embedding layer. This results in a tensor of shape <code>[batch_size, text_seq_len, embedding_dim]</code>.</li>
<li><strong>Image Embeddings:</strong> If images are present:
<ul>
<li>The processed image views (global and local) are passed through the <code>VisionTransformer</code> (SigLIP ViT). This produces a sequence of visual features for each view.</li>
<li>The visual features are then passed through the <code>MlpProjector</code>, which projects them into the same embedding space as the text embeddings.</li>
<li>Special tokens (like newline characters and view separators) are added to the sequence of image embeddings to help the model understand the spatial relationships between the image patches.</li>
<li>The image embeddings are then <em>inserted</em> into the text embedding sequence, replacing the <code>&lt;image&gt;</code> placeholder tokens. This uses the <code>masked_scatter_</code> operation, guided by the <code>images_seq_mask</code>.</li>
</ul>
</li>
<li><strong>Combined Embeddings:</strong> The result is a single tensor, <code>inputs_embeds</code>, of shape <code>[batch_size, total_seq_len, embedding_dim]</code>, where <code>total_seq_len</code> is the combined length of the text tokens and the image tokens.</li>
</ul>
</li>
<li>
<p><strong>Incremental Prefilling (DeepseekVLV2ForCausalLM.incremental_prefilling) - OPTIONAL, but highly recommended:</strong></p>
<ul>
<li>This is an optimization step. Instead of processing the entire <code>inputs_embeds</code> tensor at once, it processes it in chunks.</li>
<li>For each chunk:
<ul>
<li>A forward pass is performed through the <em>entire</em> language model (all Transformer layers).</li>
<li>The key-value pairs (from the self-attention mechanism in each Transformer layer) are <em>cached</em>.</li>
</ul>
</li>
<li>The cached key-value pairs (<code>past_key_values</code>) and the <code>inputs_embeds</code> are returned.</li>
</ul>
</li>
</ol>
<p><strong>II. Autoregressive Generation (Token-by-Token)</strong></p>
<p>This phase generates the text output, one token at a time.</p>
<ol>
<li>
<p><strong>Initial Input:</strong></p>
<ul>
<li>If <code>incremental_prefilling</code> was used, the input to the generation loop is:
<ul>
<li>The <code>inputs_embeds</code> (but now only including the <em>last</em> token, the rest represented by cache).</li>
<li>The cached <code>past_key_values</code> from prefilling.</li>
</ul>
</li>
<li>If <code>incremental_prefilling</code> was <em>not</em> used, the input is the full <code>inputs_embeds</code> tensor (and <code>past_key_values</code> is initially <code>None</code>).</li>
</ul>
</li>
<li>
<p><strong>Forward Pass (DeepseekVLV2ForCausalLM.forward within generate):</strong> The language model performs a forward pass:</p>
<ul>
<li>The <code>inputs_embeds</code> (representing either the last token or the full sequence, depending on whether prefilling was used) and the <code>past_key_values</code> (if available) are passed to the language model.</li>
<li>The Transformer layers process the input, using the cached key-value pairs to avoid redundant calculations.</li>
<li>The output of the final Transformer layer is passed through a Layer Normalization.</li>
<li>The result is a tensor of logits, of shape <code>[batch_size, 1, vocab_size]</code> (if caching is used) or <code>[batch_size, sequence_length, vocab_size]</code> (if no caching). These logits represent the model&rsquo;s predicted probability distribution over the vocabulary for the <em>next</em> token.</li>
</ul>
</li>
<li>
<p><strong>Sampling:</strong></p>
<ul>
<li>The logits are used to sample the next token. This sampling can be done in various ways (greedy decoding, top-k sampling, nucleus sampling, etc.), controlled by parameters like <code>temperature</code> and <code>top_p</code>.</li>
<li>The sampled token is represented by its ID.</li>
</ul>
</li>
<li>
<p><strong>Update:</strong></p>
<ul>
<li>The new token ID is appended to the sequence.</li>
<li>The key-value pairs calculated in the current forward pass are added to the <code>past_key_values</code> cache (or replace the existing cache if not using incremental prefilling).</li>
</ul>
</li>
<li>
<p><strong>Decoding:</strong> The generated token ID is converted back into text (a word or sub-word) using the tokenizer.</p>
</li>
<li>
<p><strong>Iteration:</strong> Steps 2-5 are repeated until a stopping criterion is met:</p>
<ul>
<li>A special end-of-sequence (EOS) token is generated.</li>
<li>A maximum sequence length is reached.</li>
<li>A stop word is generated.</li>
</ul>
</li>
<li>
<p><strong>Output:</strong> The decoded text is the model&rsquo;s response.</p>
</li>
</ol>
<p><strong>In Summary:</strong></p>
<p>The autoregressive process is a loop.  Each iteration, the model takes the <em>current</em> sequence (either the initial prompt or the prompt plus previously generated tokens), processes it (using caching to avoid redundancy), predicts the <em>next</em> token, and adds that token to the sequence. This continues until the model decides to stop. The &ldquo;autoregressive&rdquo; part means that each new token depends on all the tokens that came before it. The &ldquo;prefilling&rdquo; stage is just an optimized way of processing the initial prompt. The Vision Encoder and Projector are used <em>only</em> during the prompt processing stage to incorporate image information. Once the image embeddings are inserted into the <code>inputs_embeds</code>, the rest of the process is handled by the language model.</p>
<pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">graph LR
    subgraph User Code [&#34;User Code (Python Frontend)&#34;]
        direction TB
        A[SGLang Program] --&gt; B(Compiler)
        B --&gt; C(Interpreter)
    end

    subgraph Backend [SGLang Backend]
        direction TB
        C --&gt; D{Backend Selection}
        D --&gt;|SGLang Runtime| E[SGLang Runtime]
        D --&gt;|OpenAI API| F[OpenAI API]
        D --&gt;|Anthropic API| G[Anthropic API]
        D --&gt;|LiteLLM| H[LiteLLM]
        D --&gt;|VertexAI| I[VertexAI]
        D --&gt;|Custom Backend| J[Custom Backend]
    end
    
        subgraph SGLang_Runtime [&#34;SGLang Runtime (SRT)&#34;]
            direction TB
             SR1[Request] --&gt; SR2(Scheduler)
             SR2 --&gt; SR3(Tokenizer Manager)
             SR3 --&gt; SR4(Session Controller)
             SR4 --&gt; SR5(Data Parallel Controller)
             SR5 --&gt; SR6(Cache Controller)
             SR6 --&gt; SR7(Model Executor)
             SR7 --&gt; SR8(Detokenizer Manager)
             SR8 --&gt; SR9(Response)
        end

    subgraph Model_Executor [Model Executor]
    direction TB
        ME1[Forward Batch Info] --&gt; ME2(Model Runner)
        ME2 --&gt; ME3(CUDA Graph Runner)
        ME3 --&gt; ME4[Layers]
        ME4 --&gt; ME5((Attention, Linear, LayerNorm, ...))
        ME5 --&gt; ME6([FlashInfer, Triton, PyTorch])
        ME5 -.-&gt; ME7(Custom Kernels)
    end
    
        subgraph SGLang_Router [&#34;SGLang Router (Rust)&#34;]
            direction TB
                R1[HTTP Request] --&gt; R2(Router)
                R2 --&gt; R3(Server)
                R3 --&gt; R4(Tree)
                R4 --&gt; R5(Decision)
                R5 -.-&gt; R6[SGLang Instances]    
        end

    User_Code -.-&gt; SGLang_Router
    SGLang_Router --&gt; SGLang_Runtime
    SR7 -.-&gt; Model_Executor
    J -.-&gt; SR1

    classDef backend fill:#f9f,stroke:#333,stroke-width:2px
    classDef usercode fill:#ccf,stroke:#333,stroke-width:2px
    classDef component fill:#ddf,stroke:#333,stroke-width:1px

    class User_Code usercode
    class Backend,SGLang_Runtime,Model_Executor,SGLang_Router backend
    class A,B,C,SR1,SR2,SR3,SR4,SR5,SR6,SR7,SR8,SR9,ME1,ME2,ME3,ME4,ME5,ME6,ME7,R1,R2,R3,R4,R5,R6 component
</code></pre>

  </div>
  <div class="post-citation">
    <h1 id="citation">Citation<a hidden class="anchor" aria-hidden="true" href="#citation">#</a></h1>
    <p><br>Cited as:</p>
    <blockquote>
      <p>Michael Liu. (Jan 2024). DeepSeek-VL2. ML&#39;s Blog. http://localhost:61478/posts/deepseek-vl2/</p>
    </blockquote>
    <p>Or</p>
    <pre tabindex="0"><code>@article{DeepSeek-VL2,
  title   = "DeepSeek-VL2",
  author  = "Michael Liu",
  journal = "ML&#39;s Blog",
  year    = "2024",
  month   = "Jan",
  url     = "http://localhost:61478/posts/deepseek-vl2/"
}</code></pre>
  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:61478/">ML&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
