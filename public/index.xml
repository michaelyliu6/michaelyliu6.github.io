<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ML&#39;s Blog</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on ML&#39;s Blog</description>
    <generator>Hugo -- 0.140.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 16 Feb 2025 07:07:07 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why &#34;real&#34; Reinforcement Learning will create the strongest technical moats</title>
      <link>http://localhost:1313/posts/rl-predictions/</link>
      <pubDate>Sun, 16 Feb 2025 07:07:07 +0100</pubDate>
      <guid>http://localhost:1313/posts/rl-predictions/</guid>
      <description>&lt;p&gt;The AI landscape has undergone rapid shifts in recent years. While 2023-2024 saw the commoditization of pre-training and supervised fine-tuning, 2025 will mark the emergence of &amp;ldquo;real&amp;rdquo; Reinforcement Learning (RL) as the primary technical moat in AI development. Unlike pre-training, which focuses on learning statistical correlations from massive datasets, RL allows models to actively explore solution spaces and discover novel strategies that generalize beyond static training data.&lt;/p&gt;
&lt;h2 id=&#34;the-limitations-of-rlhf-and-the-promise-of-real-rl&#34;&gt;The Limitations of RLHF and the Promise of &amp;ldquo;Real&amp;rdquo; RL&lt;/h2&gt;
&lt;p&gt;Unlike RLHF (Reinforcement Learning from Human Feedback), which optimizes for human approval rather than actual task performance, genuine RL with sparse rewards will enable models to solve complex end-to-end tasks autonomously. RLHF is fundamentally limited because it optimizes for a proxy objective (what looks good to humans) rather than directly solving problems correctly. Furthermore, models quickly learn to game reward models when trained with RLHF for extended periods. In contrast, true RL with sparse rewards—similar to what powered AlphaGo&amp;rsquo;s breakthrough—will create significant competitive advantages for several reasons.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Training Language Models with Reinforcement Learning from Human Feedback</title>
      <link>http://localhost:1313/posts/rlhf/</link>
      <pubDate>Sat, 15 Feb 2025 07:07:07 +0100</pubDate>
      <guid>http://localhost:1313/posts/rlhf/</guid>
      <description>&lt;p&gt;Reinforcement Learning from Human Feedback (RLHF) is a technique used to fine-tune large language models (LLMs) to better align with human preferences. It involves training a reward model based on human feedback and then using reinforcement learning to optimize the LLM&amp;rsquo;s policy to maximize the reward.&lt;/p&gt;
&lt;p&gt;This process generally involves three key steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Supervised Fine-tuning (SFT):&lt;/strong&gt; An initial language model is fine-tuned on a dataset of high-quality demonstrations, where the model learns to imitate the provided examples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Toy Diffusion Model</title>
      <link>http://localhost:1313/posts/toy_diffusion_model/</link>
      <pubDate>Fri, 20 Dec 2024 07:07:07 +0100</pubDate>
      <guid>http://localhost:1313/posts/toy_diffusion_model/</guid>
      <description>&lt;h2 id=&#34;what-even-is-diffusion&#34;&gt;What even is Diffusion?&lt;/h2&gt;
&lt;p&gt;Diffusion models approach generative modeling by mapping out probability distributions in high-dimensional spaces. Consider our dataset as a tiny sample from an enormous space of possible images. Our goal is to estimate which regions of this vast space have high probability according to our target distribution.&lt;/p&gt;
&lt;p&gt;The core insight of diffusion is that if we add Gaussian noise to an image from our distribution, the resulting noisy image typically becomes less likely to belong to that distribution. This is an empirical observation about human perception - a shoe with a small amount of noise still looks like a shoe, but becomes less recognizable as more noise is added.&lt;/p&gt;</description>
    </item>
    <item>
      <title>About Me</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Sat, 17 Feb 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/about/</guid>
      <description>&lt;p&gt;Hi there! I&amp;rsquo;m Michael Liu, a software engineer passionate about AI development and safety. I combine practical engineering experience with a deep interest in advancing AI technology in reliable and beneficial ways.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m currently seeking roles in AI development.&lt;/p&gt;
&lt;h2 id=&#34;professional-background&#34;&gt;Professional Background&lt;/h2&gt;
&lt;p&gt;I most recently served as a Software Development Engineer II at Amazon Web Services (AWS), where I was a key member of the Data Transfer Metering team. Our system processed 140 PB of data and $11 million in revenue daily, serving as a critical component of AWS&amp;rsquo;s infrastructure. During my tenure, I:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning Resources</title>
      <link>http://localhost:1313/learning-resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/learning-resources/</guid>
      <description>&lt;h2 id=&#34;timeless-essaysvideos&#34;&gt;Timeless Essays/Videos&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The Bitter Lesson - &lt;a href=&#34;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&#34;&gt;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;most important century&amp;rdquo; blog post series - &lt;a href=&#34;https://www.cold-takes.com/most-important-century&#34;&gt;https://www.cold-takes.com/most-important-century&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Situational Awareness - &lt;a href=&#34;https://situational-awareness.ai/&#34;&gt;https://situational-awareness.ai/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Machines of Loving Grace - &lt;a href=&#34;https://darioamodei.com/machines-of-loving-grace&#34;&gt;https://darioamodei.com/machines-of-loving-grace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stanford CS25: V4 I Hyung Won Chung of OpenAI (2:05 - 15:18) - &lt;a href=&#34;https://youtu.be/orDKvo8h71o?si=oFd2l7YQzOoGat1q&#34;&gt;https://youtu.be/orDKvo8h71o?si=oFd2l7YQzOoGat1q&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AlphaGo Documentary - &lt;a href=&#34;https://youtu.be/WXuK6gekU1Y?si=uuPxNmTrpUNwzPA5&#34;&gt;https://youtu.be/WXuK6gekU1Y?si=uuPxNmTrpUNwzPA5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Andrew Ng&amp;rsquo;s Career Advice / Reading Research Papers &lt;a href=&#34;https://youtu.be/733m6qBH-jI?si=OLaGzeqvuObpjl9H&#34;&gt;https://youtu.be/733m6qBH-jI?si=OLaGzeqvuObpjl9H&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;youtube-channelspodcasts&#34;&gt;Youtube Channels/Podcasts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dwarkesh Patel - &lt;a href=&#34;https://www.youtube.com/c/DwarkeshPatel&#34;&gt;https://www.youtube.com/c/DwarkeshPatel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AI Explained - &lt;a href=&#34;https://www.youtube.com/@aiexplained-official&#34;&gt;https://www.youtube.com/@aiexplained-official&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Latent Space - &lt;a href=&#34;https://www.youtube.com/@LatentSpacePod&#34;&gt;https://www.youtube.com/@LatentSpacePod&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;80 000 hours - &lt;a href=&#34;https://www.youtube.com/@eightythousandhours&#34;&gt;https://www.youtube.com/@eightythousandhours&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cognitive Revolution - &lt;a href=&#34;https://www.youtube.com/@CognitiveRevolutionPodcast&#34;&gt;https://www.youtube.com/@CognitiveRevolutionPodcast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neel Nanda - &lt;a href=&#34;https://www.youtube.com/@neelnanda2469&#34;&gt;https://www.youtube.com/@neelnanda2469&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;courses&#34;&gt;Courses&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Alignment Research Engineer Accelerator (ARENA) - &lt;a href=&#34;https://github.com/callummcdougall/ARENA_3.0&#34;&gt;https://github.com/callummcdougall/ARENA_3.0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hugging Face - &lt;a href=&#34;https://huggingface.co/courses&#34;&gt;https://huggingface.co/courses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Advanced Large Language Model Agents: Berkley MOOC - &lt;a href=&#34;https://llmagents-learning.org/sp25&#34;&gt;https://llmagents-learning.org/sp25&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Andrej Karpathy&amp;rsquo;s Zero to Hero Series - &lt;a href=&#34;https://karpathy.ai/zero-to-hero.html&#34;&gt;https://karpathy.ai/zero-to-hero.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;From Deep Learning Foundations to Stable Diffusion - &lt;a href=&#34;https://course.fast.ai/Lessons/part2.html&#34;&gt;https://course.fast.ai/Lessons/part2.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DeepLearning.AI specializations (short courses are skippable imo) - &lt;a href=&#34;https://www.deeplearning.ai/&#34;&gt;https://www.deeplearning.ai/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;blogs&#34;&gt;Blogs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GOATs
&lt;ul&gt;
&lt;li&gt;Gwern - &lt;a href=&#34;https://www.gwern.net/&#34;&gt;https://www.gwern.net/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lillian Weng&amp;rsquo;s Lil&amp;rsquo; Log - &lt;a href=&#34;https://lilianweng.github.io/&#34;&gt;https://lilianweng.github.io/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Alignment and Interpretability
&lt;ul&gt;
&lt;li&gt;Anthropic&amp;rsquo;s Alignment Science Blog - &lt;a href=&#34;https://alignment.anthropic.com/&#34;&gt;https://alignment.anthropic.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Anthropic&amp;rsquo;s Transformer Circuits Thread - &lt;a href=&#34;https://transformer-circuits.pub/&#34;&gt;https://transformer-circuits.pub/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LessWrong - &lt;a href=&#34;https://www.lesswrong.com/&#34;&gt;https://www.lesswrong.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Reinforcement Learning
&lt;ul&gt;
&lt;li&gt;RLHF Book - &lt;a href=&#34;https://rlhfbook.com/&#34;&gt;https://rlhfbook.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Industry Trends
&lt;ul&gt;
&lt;li&gt;Epoch AI - &lt;a href=&#34;https://epoch.ai/blog&#34;&gt;https://epoch.ai/blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SemiAnalysis - &lt;a href=&#34;https://www.semianalysis.com/&#34;&gt;https://www.semianalysis.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Agents
&lt;ul&gt;
&lt;li&gt;Anthropic: Building effective agents - &lt;a href=&#34;https://www.anthropic.com/research/building-effective-agents&#34;&gt;https://www.anthropic.com/research/building-effective-agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Google: Agents Whitepaper - &lt;a href=&#34;https://www.kaggle.com/whitepaper-agents&#34;&gt;https://www.kaggle.com/whitepaper-agents&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;LLM Training
&lt;ul&gt;
&lt;li&gt;FineWeb - &lt;a href=&#34;https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1&#34;&gt;https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;How to Scale Your Model: A Systems View of LLMs on TPUs - &lt;a href=&#34;https://jax-ml.github.io/scaling-book/&#34;&gt;https://jax-ml.github.io/scaling-book/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The Ultra-Scale Playbook: Training LLMs on GPU Clusters - &lt;a href=&#34;https://huggingface.co/spaces/nanotron/ultrascale-playbook&#34;&gt;https://huggingface.co/spaces/nanotron/ultrascale-playbook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;github-repos&#34;&gt;Github Repos&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;System Prompts
&lt;ul&gt;
&lt;li&gt;Leaked System Prompts - &lt;a href=&#34;https://github.com/jujumilk3/leaked-system-prompts&#34;&gt;https://github.com/jujumilk3/leaked-system-prompts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;LLM Inference
&lt;ul&gt;
&lt;li&gt;SGLang - &lt;a href=&#34;https://github.com/sgl-project/sglang&#34;&gt;https://github.com/sgl-project/sglang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;vLLM - &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;https://github.com/vllm-project/vllm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;TensorRT - &lt;a href=&#34;https://github.com/NVIDIA/TensorRT&#34;&gt;https://github.com/NVIDIA/TensorRT&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Reinforcement Learning
&lt;ul&gt;
&lt;li&gt;verl - &lt;a href=&#34;https://github.com/volcengine/verl&#34;&gt;https://github.com/volcengine/verl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;trl - &lt;a href=&#34;https://github.com/huggingface/trl&#34;&gt;https://github.com/huggingface/trl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Search-R1 - &lt;a href=&#34;https://github.com/PeterGriffinJin/Search-R1&#34;&gt;https://github.com/PeterGriffinJin/Search-R1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Agents
&lt;ul&gt;
&lt;li&gt;smolagents - &lt;a href=&#34;https://github.com/huggingface/smolagents&#34;&gt;https://github.com/huggingface/smolagents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenHands (formerly OpenDevin) - &lt;a href=&#34;https://github.com/All-Hands-AI/OpenHands&#34;&gt;https://github.com/All-Hands-AI/OpenHands&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;textbooks&#34;&gt;Textbooks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Reinforcement Learning by Richard S. Sutton and Andrew G. Barto - &lt;a href=&#34;https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf&#34;&gt;https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Generative AI System Design Interview - &lt;a href=&#34;https://www.amazon.com/Generative-AI-System-Design-Interview/dp/1736049143&#34;&gt;https://www.amazon.com/Generative-AI-System-Design-Interview/dp/1736049143&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;newsletters&#34;&gt;Newsletters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;AI News - &lt;a href=&#34;https://buttondown.com/ainews/&#34;&gt;https://buttondown.com/ainews/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Latent Space - &lt;a href=&#34;https://www.latent.space/&#34;&gt;https://www.latent.space/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;China Talk - &lt;a href=&#34;https://www.chinatalk.media/&#34;&gt;https://www.chinatalk.media/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Import AI by Jack Clark - &lt;a href=&#34;https://importai.substack.com/&#34;&gt;https://importai.substack.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rohan&amp;rsquo;s Bytes - &lt;a href=&#34;https://rohanpaul.substack.com/&#34;&gt;https://rohanpaul.substack.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;tools&#34;&gt;Tools&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Cursor - &lt;a href=&#34;https://www.cursor.com/&#34;&gt;https://www.cursor.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;alphaXiv - &lt;a href=&#34;https://www.alphaxiv.org/explore&#34;&gt;https://www.alphaxiv.org/explore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gitingest - &lt;a href=&#34;https://gitingest.com/&#34;&gt;https://gitingest.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;leetcode&#34;&gt;LeetCode&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;NeetCode - &lt;a href=&#34;https://neetcode.io/&#34;&gt;https://neetcode.io/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-this-page&#34;&gt;What is this page?&lt;/h2&gt;
&lt;p&gt;This is my personal collection of AI learning resources that I&amp;rsquo;ve found valuable in my journey. While it primarily serves as my digital garden of knowledge links (which I keep permanently bookmarked for easy reference), I&amp;rsquo;m sharing it publicly for two important reasons:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Projects</title>
      <link>http://localhost:1313/projects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/</guid>
      <description>&lt;h2 id=&#34;gpt-2-transformer-implementation-and-mechanistic-interpretability-experimentation&#34;&gt;GPT-2 Transformer Implementation and Mechanistic Interpretability Experimentation&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&#34;GPT-2 Transformer&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/assets/images/gpt2-transformer-image.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;A comprehensive implementation and exploration of transformer-based language models, focusing on GPT-2 architecture and mechanistic interpretability. This project features three main components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a clean, educational GPT-2 implementation from scratch with detailed documentation and intuition&lt;/li&gt;
&lt;li&gt;a production-ready lightweight implementation with efficient training pipelines&lt;/li&gt;
&lt;li&gt;advanced mechanistic interpretability tools for analyzing model internals including attention patterns, feature representations, and circuit behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The codebase demonstrates expertise in PyTorch, transformer architecture design, natural language processing techniques, and cutting-edge interpretability methods for understanding language model internals.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
